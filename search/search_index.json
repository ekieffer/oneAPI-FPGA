{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to FPGA programming with Intel\u00ae oneAPI","text":"<p>Intel\u00ae oneAPI is a software development toolkit from Intel designed to simplify the process of developing high-performance applications for various types of computing architecture. It aims to provide a unified and simplified programming model for CPUs, GPUs, FPGAs, and other types of hardware, such as AI accelerators, allowing developers to use a single codebase for multiple platforms.</p> <p>One of the main components of oneAPI is the Data Parallel C++ (DPC++), an open, standards-based language built upon the ISO C++ and SYCL standards. DPC++ extends C++ with features like parallel programming constructs and heterogeneous computing support, providing developers with the flexibility to write code for different types of hardware with relative ease.</p> <p>In addition to DPC++, oneAPI includes a range of libraries designed to optimize specific types of tasks, such as machine learning, linear algebra, and deep learning. These include oneDNN for deep neural networks, oneMKL for math kernel library, and oneDAL for data analytics, among others.</p> <p>It's important to note that Intel oneAPI is part of Intel's broader strategy towards open, standards-based, cross-architecture programming, which is intended to reduce the complexity of application development and help developers leverage the capabilities of different types of hardware more efficiently and effectively.</p> <p>In this course, you will learn to:</p> <ul> <li>Use the DPC++ compiler to create executable for Intel FPGA hardware</li> <li>Discover the SYCL C++ abstraction layer</li> <li>How to move data from and to FPGA hardware</li> <li>Optimize FPGA workflows</li> </ul> <p>Remark</p> <p>This course is not intended to be exhaustive. In addition, the described tools and features are constantly evolving. We try our best to keep it up to date. </p>"},{"location":"#who-is-the-course-for","title":"Who is the course for ?","text":"<p>This course is for students, researchers, enginners wishing to discover how to use oneAPI to program FPGA. This course do not requires any knowledge of SYCL/DPC++, however, participants should have some experience with modern C++. Lambdas, class deduction templates, etc ... should be at least known before digging into this course. </p> <p>We recommend the excellent book: \"Professional C++ (5th Edition)\" by Marc Gregoire.</p>"},{"location":"#about-this-course","title":"About this course","text":"<p>This course has been developed in the context of the EuroCC National Competence Center Luxembourg. </p>"},{"location":"compile/","title":"Compiling SYCL programs for Intel\u00ae FPGA cards","text":""},{"location":"compile/#setups","title":"Setups","text":"<p>Please clone first the oneAPI-sample repository with the <code>git clone https://github.com/oneapi-src/oneAPI-samples.git</code> in your home folder.</p> <p>Once the repository cloned, you should see the following hierarchy:</p> <pre><code>$ tree -d -L 2 oneAPI-samples\noneAPI-samples\n\u251c\u2500\u2500 AI-and-Analytics\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 End-to-end-Workloads\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Features-and-Functionality\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Getting-Started-Samples\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 images\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Jupyter\n\u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 stb\n\u251c\u2500\u2500 DirectProgramming\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 C++\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 C++SYCL\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 C++SYCL_FPGA\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Fortran\n\u251c\u2500\u2500 Libraries\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oneCCL\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oneDAL\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oneDNN\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oneDPL\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oneMKL\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 oneTBB\n\u251c\u2500\u2500 Publications\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 DPC++\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 GPU-Opt-Guide\n\u251c\u2500\u2500 RenderingToolkit\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 GettingStarted\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Tutorial\n\u251c\u2500\u2500 Templates\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 cmake\n\u2514\u2500\u2500 Tools\n    \u251c\u2500\u2500 Advisor\n    \u251c\u2500\u2500 ApplicationDebugger\n    \u251c\u2500\u2500 Benchmarks\n    \u251c\u2500\u2500 GPU-Occupancy-Calculator\n    \u251c\u2500\u2500 Migration\n    \u2514\u2500\u2500 VTuneProfiler\n</code></pre> <ul> <li>As you can see Intel provides numerous code samples and examples to help your grasping the power of the oneAPI toolkit. </li> <li>We are going to focus on <code>DirectProgramming/C++SYCL_FPGA</code>.</li> <li> <p>Create a symbolic at the root of your home directory pointing to this folder: <pre><code>$ cd\n$ ln -s oneAPI-samples/DirectProgramming/C++SYCL_FPGA/Tutorials/GettingStarted\n$ tree -d -L 2 GettingStarted\nGettingStarted\n\u251c\u2500\u2500 fast_recompile\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 assets\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 src\n\u251c\u2500\u2500 fpga_compile\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part1_cpp\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part2_dpcpp_functor_usm\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 part3_dpcpp_lambda_usm\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 part4_dpcpp_lambda_buffers\n\u2514\u2500\u2500 fpga_template\n    \u2514\u2500\u2500 src\n</code></pre></p> </li> <li> <p>The fpga_compile folder provides basic examples to start compiling SYCL C++ code with the DPC++ compiler</p> </li> <li> <p>The fpga_recompile folder show you how to recompile quickly your code without having to rebuild the FPGA image</p> </li> <li> <p>The fpga_template is a starting template project that you can use to bootstrap a project</p> </li> </ul>"},{"location":"compile/#discovering-devices","title":"Discovering devices","text":"<p>Before targeting a specific hardware accelerator, you need to ensure that the sycl runtime is able to detect it.</p> <p>Commands</p> <pre><code># Create permanent tmux session\ntmux new -s fpga_session\n# We need a job allocation on a FPGA node\nsalloc -A p200117 -t 48:00:00 -q default -p fpga -N 1\n# In order to use the  intel-compiler-2023.2.1\nmodule use /project/home/p200117/apps/u100057/easybuild/modules/all\nmodule load 520nmx/20.4\n# The fpga_compile version setup all necessary environment variable to compile code\nmodule load intel-compilers/2023.2.1-fpga_compile\nsycl-ls\n</code></pre> <p>Output</p> <pre><code>[opencl:cpu:0] Intel(R) OpenCL, AMD EPYC 7452 32-Core Processor                 3.0 [2022.13.3.0.16_160000]\n[opencl:acc:1] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device 1.2 [2022.13.3.0.16_160000]\n[opencl:acc:2] Intel(R) FPGA SDK for OpenCL(TM), p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0) 1.0 [2022.1]\n[opencl:acc:3] Intel(R) FPGA SDK for OpenCL(TM), p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie1) 1.0 [2022.1]\n</code></pre> <ul> <li>If you see the same output, you are all setup.</li> </ul>"},{"location":"compile/#compilation-manually","title":"Compilation (manually)","text":"<ul> <li>Recalling that full compilation can take hours depending on your application size.</li> <li>In this context, emulation and static report evaluation are keys to succeed in FPGA programming</li> </ul> <p>Full compilation &amp; hardware profiling</p> <p>Don't try a classical debug approach while hoping to solve a problem using multiple design iterations in this condition.  HLS-FPGA programming can be very tedious but SYCL simplifies greatly the process.   </p>"},{"location":"compile/#first-code","title":"First code","text":"<p>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src/vector_add.cpp</p> <pre><code>#include &lt;iostream&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\nvoid VectorAdd(const int *vec_a_in, const int *vec_b_in, int *vec_c_out,\nint len) {\nfor (int idx = 0; idx &lt; len; idx++) {\nint a_val = vec_a_in[idx];\nint b_val = vec_b_in[idx];\nint sum = a_val + b_val;\nvec_c_out[idx] = sum;\n}\n}\nconstexpr int kVectSize = 256;\nint main() {\nbool passed = true;\ntry {\n// Use compile-time macros to select either:\n//  - the FPGA emulator device (CPU emulation of the FPGA)\n//  - the FPGA device (a real FPGA)\n//  - the simulator device\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// create the device queue\nsycl::queue q(selector);\n// make sure the device supports USM host allocations\nauto device = q.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\n// declare arrays and fill them\nint * vec_a = new int[kVectSize];\nint * vec_b = new int[kVectSize];\nint * vec_c = new int[kVectSize];\nfor (int i = 0; i &lt; kVectSize; i++) {\nvec_a[i] = i;\nvec_b[i] = (kVectSize - i);\n}\nstd::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n{\n// copy the input arrays to buffers to share with kernel\nsycl::buffer buffer_a{vec_a, sycl::range(kVectSize)};\nsycl::buffer buffer_b{vec_b, sycl::range(kVectSize)};\nsycl::buffer buffer_c{vec_c, sycl::range(kVectSize)};\nq.submit([&amp;](sycl::handler &amp;h) {\n// use accessors to interact with buffers from device code\nsycl::accessor accessor_a{buffer_a, h, sycl::read_only};\nsycl::accessor accessor_b{buffer_b, h, sycl::read_only};\nsycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\nh.single_task&lt;VectorAddID&gt;([=]() {\nVectorAdd(&amp;accessor_a[0], &amp;accessor_b[0], &amp;accessor_c[0], kVectSize);\n});\n});\n}\n// result is copied back to host automatically when accessors go out of\n// scope.\n// verify that VC is correct\nfor (int i = 0; i &lt; kVectSize; i++) {\nint expected = vec_a[i] + vec_b[i];\nif (vec_c[i] != expected) {\nstd::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; vec_c[i] &lt;&lt; \", expected (\"\n&lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; vec_b[i]\n&lt;&lt; std::endl;\npassed = false;\n}\n}\nstd::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\ndelete[] vec_a;\ndelete[] vec_b;\ndelete[] vec_c;\n} catch (sycl::exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li> <p>The <code>vector_add.cpp</code> source file contains all the necessary to understand how to create a SYCL program</p> </li> <li> <p>lines 4 and 5 are the minimal headers to include in your SYCL program</p> </li> <li> <p>line 9 is a forward declaration of the kernel name</p> </li> <li> <p>lines 11-19 is a function representing our kernel. Note the absence of <code>__kernel</code>, <code>__global</code> as it exists in OpenCL</p> </li> <li> <p>lines 30-36 are pragmas defining whether you want a full compilation, a CPU emulation or the simulator</p> </li> <li> <p>line 39 is the queue creation. The queue is bounded to a device. We will discuss it later in details.</p> </li> <li> <p>lines 41-46 provides debugging information at runtime.</p> </li> <li> <p>lines 48-54 instantiates 3 vectors. <code>vec_a</code> and <code>vec_b</code> are input C++ arrays and are initialized inside the next loop. <code>vec_c</code> is an output C++ array collecting computation results between <code>vec_a</code> and <code>vec_b</code>.</p> </li> <li> <p>lines 60-62 create buffers for each vector and specify their size. The runtime copies the data to the FPGA global memory when the kernel starts</p> </li> <li> <p>line 64 submits a command group to the device queue</p> </li> <li> <p>lines 66-68 relies on accessor to infer data dependencies. \"read_only\" accessor have to wait for data to be fetched. \"no_init\" option indicates ito the runtime know that the previous contents of the buffer can be discarded</p> </li> <li> <p>lines 70-73 starts a single tasks (single work-item) and call the kernel function</p> </li> <li> <p>lines 99-105 catch SYCL exceptions and terminate the execution</p> </li> </ul>"},{"location":"compile/#emulation","title":"Emulation","text":"<ul> <li> <p>FPGA emulation refers to the process of using a software or hardware system to mimic the behavior of an FPGA device. This is usually done to test, validate, and debug FPGA designs before deploying them on actual hardware. The Intel\u00ae FPGA emulator runs the code on the host cpu.</p> </li> <li> <p>Emulation is crucial to validate the functionality of your kernel design. </p> </li> <li> <p>During emulation, your are not seeking for performance.</p> </li> </ul> <p>Compile for emulation (in one step)</p> <pre><code>$ icpx -fsycl -fintelfpga -qactypes vector_add.cpp -o vector_add.fpga_emu\n</code></pre> <p>Intel uses the SYCL Ahead-of-time (AoT) compilation which as two steps:</p> <ol> <li> <p>The \"compile\" stage compiles the device code to an intermediate representation (SPIR-V).</p> </li> <li> <p>The \"link\" stage invokes the compiler's FPGA backend before linking.</p> </li> </ol> <p>Two-steps compilation</p> <pre><code># Compile \n$ icpx -fsycl -fintelfpga -qactypes -o vector_add.cpp.o -c vector_add.cpp\n# Link\n$ icpx -fsycl -fintelfpga -qactypes vector_add.cpp.o -o vector_add.fpga_emu\n</code></pre> <p>Practical session</p> <ul> <li>Go to the <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code></li> <li>Apply the previous command (single or two steps). What do you read ?    </li> </ul> <ul> <li>The compiler option <code>-qactypes</code> informs the compiler to sreahc and include the Algorithmic C (AC) data type folder for header and libs to the AC data types libraries for Field Programmable Gate Array (FPGA) and CPU compilations.</li> <li>The Algorithmic C (AC) datatypes libraries include a numerical set of datatypes and an interface datatype for modeling channels in communicating processes in C++.</li> </ul>"},{"location":"compile/#static-reports","title":"Static reports","text":"<ul> <li> <p>During the process of compiling an FPGA hardware image with the Intel\u00ae oneAPI DPC++/C++ Compiler, various checkpoints are provided at different compilation steps. These steps include object files generation, an FPGA early image object generation, an FPGA image object generation, and finally executables generation. These checkpoints offer the ability to review errors and make modifications to the source code without needing to do a full compilation every time. </p> </li> <li> <p>When you reach the FPGA early image object checkpoint, you can examine the optimization report generated by the compiler. </p> </li> <li> <p>Upon arriving at the FPGA image object checkpoint, the compiler produces a finished FPGA image.</p> </li> </ul> <p>In order to generate the FPGA early image, you will need to add the following option:</p> <ul> <li> <p><code>-Xshardware</code></p> </li> <li> <p><code>-Xstarget=&lt;target&gt;</code> or <code>-Xsboard=&lt;board&gt;</code></p> </li> <li> <p><code>-fsycl-link=early</code></p> </li> </ul> <p>Compile for FPGA early image</p> <pre><code>$ icpx -fsycl -fintelfpga -qactypes -Xshardware -fsycl-link=early -Xstarget=Stratix10 vector_add.cpp -o vector_add_report.a\n</code></pre> <ul> <li>The <code>vector_add_report.a</code> is not what we target in priority. We target the reports directory <code>vector_add_report.prj</code> which has been created.</li> </ul> <p>Practical session</p> <ul> <li>Go to the <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code></li> <li>Apply the previous command to the single compilation command</li> </ul> <ul> <li> <p>You can evaluate whether the estimated kernel performance data is satisfactory by going to the /reports/ directory and examining one of the following files related to your application: <li> <p>report.html: This file can be viewed using Internet browsers of your choice</p> </li> <li>.zip: Utilize the Intel\u00ae oneAPI FPGA Reports tool,i.e., <code>fpga_report</code>"},{"location":"compile/#full-compilation","title":"Full compilation","text":"<p>This phase produces the actual FPGA bitstream, i.e., a file containing the programming data associated with your FPGA chip. This file requires the target FPGA platform to be generated and executed. For FPGA programming, the Intel\u00ae oneAPI toolkit requires the Intel\u00ae Quartus\u00ae Prime software to generate this bitstream.</p> <p>Full hardware compilation</p> <pre><code>$ icpx -fsycl -fintelfpga -qactypes -Xshardware -Xstarget=Stratix10 -DFPGA_HARDWARE vector_add.cpp -o vector_add_report.fpga\n</code></pre> <ul> <li> <p>The compilation will take several hours. Therefore, we strongly advise you to verify your code through emulation first.</p> </li> <li> <p>You can also use the <code>-Xsfast-compile</code> option which offers a faster compile time but reduce the performance of the final FPGA image.</p> </li> </ul>"},{"location":"compile/#fast-recompilation","title":"Fast recompilation","text":"<ul> <li> <p>At first glance having a single source file is not necessarily a good idea when host and device compilation differs so much</p> </li> <li> <p>However, there is two different strategies to deal with it:</p> </li> <li> <p>Use a single source file and add the <code>-reuse-exe</code></p> </li> <li> <p>Separate host and device code compilation in your FPGA project</p> </li> <li> <p>This is up to you to choose the method that suits you the most</p> </li> </ul> <p>Using the <code>-reuse-exe</code> option</p> <p><pre><code>$ icpx -fsycl -fintelfpga -qactypes -Xshardware -Xstarget=Stratix10 -DFPGA_HARDWARE -reuse-exe=vector_add.fpga vector_add.cpp -o vector_add.fpga\n</code></pre> If only the host code changed since the previous compilation, providing the <code>-reuse-exe=image</code> flag to <code>icpx</code> instructs the compiler to extract the compiled FPGA binary from the existing executable and package it into the new executable, saving the device compilation time.</p> <p>Question</p> <ul> <li>What happens if the vector_add.fpga is missing ?</li> </ul> <p>Separating host and device code</p> <p>Go to the <code>GettingStarted/fpga_recompile</code> folder. It provides an example of separate host and device code The process is similar as the compilation process for OpenCL except that a single tool is used, i.e., <code>icpx</code></p> <ol> <li>Compile the host code: <pre><code>$ icpx -fsycl -fintelfpga -DFPGA_HARDWARE host.cpp -c -o host.o\n</code></pre></li> <li>Compile the FPGA image: <pre><code>$ icpx -fsycl -fintelfpga -Xshardware -Xstarget=Stratix10 -fsycl-link=image kernel.cpp -o dev_image.a\n</code></pre></li> <li>Link both: <pre><code>$ icpx -fsycl -fintelfpga host.o dev_image.a -o fast_recompile.fpga\n</code></pre></li> </ol>"},{"location":"compile/#summary","title":"Summary","text":"<p>We have seen</p> <ul> <li>How to discover devices on your system</li> <li>How to manually compile a SYCL program<ul> <li>for emulation</li> <li>for early reporting  </li> <li>full hardware compilation</li> </ul> </li> <li>How to perform fast recompilation</li> </ul> <p>We did not see</p> <ul> <li>Compilation for hardware simulation</li> <li>Advanced compilation with multiple source files</li> <li>Extract the bitstream from the final executable</li> </ul>"},{"location":"dpcpp/","title":"What is the Intel\u00ae oneAPI DPC++ compiler","text":"<p>In heterogenous computing, accelerator devices support the host processor by executing specific portion of code more efficiently. In this context, the Intel\u00ae oneAPI toolkit supports two different approach for heterogeous computing:</p> <p>1. Data Parallel C++ with SYCL</p> <p>SYCL (Specification for Unified Cross-platform C++) provides a higher-level model for writing standard ISO C++ code that is both performance-oriented and portable across various hardware, including CPUs, GPUs and FPGAs It enables the use of standard C++ with extensions to leverage parallel hardware. Host and kernel code share the same source file. The DPC++ compiler is adding SYCL support on top of the LLVM C++ compiler. DPC++ is distributed with the Intel\u00ae oneAPI toolkit.</p> <p>2. OpenMP for C, C++, and Fortran </p> <p>For more than two decades, OpenMP has stood as a standard programming language, with Intel implementing its 5th version. The Intel oneAPI C++ Compiler, which includes support for OpenMP offloading, can be found in the Intel oneAPI Base Toolkit, Intel oneAPI HPC Toolkit, and Intel oneAPI IoT Toolkit. Both the Intel\u00ae Fortran Compiler Classic and the Intel\u00ae Fortran Compiler equipped with OpenMP offload support are accessible through the Intel oneAPI HPC Toolkit. </p> <p>Note: OpenMP is not supported for FPGA devices.</p>"},{"location":"dpcpp/#dpc-is-one-of-the-existing-sycl-implementations","title":"DPC++ is one of the existing SYCL implementations","text":"<p>ComputeCpp (codeplay)</p> <p>support will no longer be provided from September 1st 2023 (see announce)</p>"},{"location":"dpcpp/#key-features-and-components","title":"Key Features and Components","text":"<ul> <li>Heterogeneous Support: Enables coding for various types of processors within the same program.</li> <li>Performance Optimization: It offers various optimization techniques to ensure code runs as efficiently as possible.</li> <li>Standard Compliance: Aligns with the latest C++ standards, along with the SYCL standard.</li> <li>Debugging and Analysis Tools: Integrates with tools that assist in debugging and analyzing code.</li> <li>Integration with IDEs: Compatible with popular Integrated Development Environments to facilitate a seamless coding experience.</li> <li>Open Source and Community Driven: This promotes collaboration and ensures that the technology stays up to date with industry needs.</li> </ul>"},{"location":"dpcpp/#sycl-and-fpga","title":"SYCL and FPGA","text":"<p>SYCL offers APIs and abstractions, but FPGA cards are unique to each vendor, and even within the same vendor, FPGA cards may have diverse capabilities. DPC++ targets Intel\u00ae FPGA cards specifically and extends SYCL's functions. This allows it to leverage the strength of FPGA, all the while maintaining as much generalizability and portability as possible.</p>"},{"location":"dpcpp/#references","title":"References","text":"<ul> <li>Data Parallel C++: Mastering DPC++ for Programming of Heterogeneous Systems using C++ and SYCL</li> <li>Intel\u00ae oneAPI DPC++/C++ Compiler </li> <li>SYCL official documentation</li> </ul>"},{"location":"intro/","title":"Introduction to FPGA computing for the HPC ecosystem","text":""},{"location":"meluxina/","title":"Using the Luxembourgish National Supercomputer Meluxina","text":"<p>If you follow this course through the workshop organized by Supercomputing Luxembourg, an allocation on Meluxina has been provided to you by LuxProvide.</p> <p>During this workshop, we will stringly rely on the National Supercomputer. Meluxina is a supercomputer located in Luxembourg, which began operation in 2021. It is part of the European High-Performance Computing (EuroHPC) Joint Undertaking, an initiative by the European Union to develop a world-class supercomputing ecosystem in Europe.</p> <p>The Meluxina supercomputer is hosted by LuxProvide, the national HPC organization in Luxembourg. Its computing power is intended to be used for a wide range of tasks, such as data visualization, artificial intelligence, and simulating complex systems, serving both academic researchers and industry. The machine's architecture includes CPUs, GPUs and FPGAs. It is designed to be energy-efficient, utilizing technologies to reduce power consumption and lower its environmental footprint.</p> <p>Meluxina's FPGA nodes have two 2 Intel Stratix 10MX 16 GB FPGA cards. The FPGA partition contains 20 nodes.</p> <p></p>"},{"location":"meluxina/#connecting-to-meluxina","title":"Connecting to Meluxina","text":"<p>You should have received your credentials some days before the workshop. If not, please contact the organizer. Please follow the LuxProvide documentation to setup your access.</p>"},{"location":"meluxina/#accessing-the-fpga-partition","title":"Accessing the fpga partition","text":"<p>The oneAPI toolkit is evolving quite rapidely. In order to show the last features, the last version has been compiled for the course/workshop.</p>"},{"location":"meluxina/#offline-compilation","title":"Offline compilation","text":"<p>In order to use the oneAPI toolkit, you first need to take an interactive job. We strongly recommend to open a persistent terminal session using <code>tmux</code></p> <p>As we are focusing on FPGA, we select the fpga partition:</p> <p><pre><code>$ tmux new -s fpga_session\n[fpga_session]$ salloc -A p200117 -t 01:00:00 -q default -p fpga -N 1\n</code></pre> Once the fpga node has been allocated to you, you can load the lastest oneAPI toolkit with  <pre><code>[fpga_session]$ module use /project/home/p200117/apps/u100057/easybuild/modules/all\n[fpga_session]$ module load intel-compilers/2023.2.1-fpga_compile\n</code></pre> The root folder containing binaries, include and lib can be located using the <code>${EBROOTINTELMINCOMPILERS}</code>. <pre><code>$ ls ${EBROOTINTELMINCOMPILERS}\ntotal 84\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 compiler\ndrwxr-xr-x 6 apps hpcusers  4096 Nov 12  2022 conda_channel\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 debugger\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 dev-utilities\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 easybuild\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 etc\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 licensing\ndrwxr-xr-x 2 apps hpcusers  4096 Nov 12  2022 logs\n-rwxr-xr-x 1 apps hpcusers 10249 Feb 16  2022 modulefiles-setup.sh\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 mpi\n-rwxr-xr-x 1 apps hpcusers 26718 Feb 11  2022 setvars.sh\n-rw-r--r-- 1 apps hpcusers   155 Sep 13  2019 support.txt\ndrwxr-xr-x 3 apps hpcusers  4096 Nov 12  2022 tbb\n</code></pre> The process of creating an FPGA hardware image necessitates the use of Intel\u00ae Quartus\u00ae Prime software to transform your design from RTL (Register Transfer Level) to the FPGA's primitive hardware resources. A FPGA BSP (Board Support Package) refers to a collection of software and hardware components designed to support and enable the development of applications on a specific FPGA board. BSPs are commonly used in FPGA development to provide an abstraction layer between the hardware and software, making it easier for developers to create and run their applications on the FPGA platform. In order work with the Bittware Stratix 520MX provided by LuxProvide, we need to load the corresponding BSP.</p> <pre><code>$ module load 520nmx/20.4\n</code></pre>"},{"location":"meluxina/#execution","title":"Execution","text":"<p>Execution does not require the hardware compiler Quartus Prime. In order to execute the FPGA binary once it has been generated, the following modules should be loaded instead:</p> <pre><code>[fpga_session]$ module purge\n[fpga_session]$ module load 520nmx/20.4\n# Don't forget to comment the next command if you have started a new tmux/screen session\n#[fpga_session]$ module use /project/home/p200117/apps/u100057/easybuild/modules/all\n[fpga_session]$ module load intel-compilers/2023.2.1-fpga_execute\n</code></pre>"},{"location":"meluxina/#emulation","title":"Emulation","text":"<p>Emulation only relies on the host cpu. In theory, you don't need to load the BSP board but you still need to <code>module load intel-compilers/2023.2.1-fpga_execute</code>.</p>"},{"location":"meluxina/#graphical-sessions-vnc","title":"Graphical sessions (VNC)","text":"<p>During this course/workshop, we will need GUI software and, more particularly,the Intel\u00ae VTune\u2122 Profiler. Using the Intel\u00ae VTune\u2122 Profiler allows you to analyze the performance of your application and identify performance bottlenecks in your code. </p> <p>You can use the profiler in interactive or batch mode. By default, the <code>vtune-gui</code> executable is started after having enabled X11 forwarding (see Meluxina's documentation). X11 forwarding can be very laggy when GUI applications are too demanding. This is the reason why we choose to use VNC.</p> <p>VNC (Virtual Network Computing) is a technology that allows you to remotely access and control a computer's desktop environment over a network connection. It enables users to view and interact with a remote computer as if they were sitting in front of it physically. VNC works by transmitting the graphical user interface (GUI) of the remote computer to the client device and relaying back user input from the client to the remote computer.</p> <p>The VNC system consists of two main components:</p> <ul> <li> <p>VNC Server: This software component runs on the computer that you want to access remotely. It captures the graphical output of the desktop environment and sends it over the network to the client.</p> </li> <li> <p>VNC Viewer: The VNC Viewer is the client software that runs on the device from which you want to access the remote computer. It receives the graphical data from the VNC server and presents it to the user, allowing them to interact with the remote desktop as if it were running locally.</p> </li> </ul> <p>Meluxina nodes do not propose a VNC server application but we can remedy to it using Singularity-CE containers. As this is not a course on container, we will not explain in details the singularity definition file.</p> <p>In order to faciliate its usage, the following launcher can be used to start a batch job.</p> <p>launcher_vtune.sh</p> <pre><code>#!/bin/bash -l\n#SBATCH --nodes=1                          # number of nodes\n#SBATCH --ntasks=2                         # number of tasks\n#SBATCH --cpus-per-task=1                  # number of cores per task\n#SBATCH --time=40:00:00                    # time (HH:MM:SS)\n#SBATCH --account=p200117                  # project account\n#SBATCH --partition=cpu                   # partition\n#SBATCH --qos=default                         # QOS\n##SBATCH --mail-user=emmanuel.kieffer@uni.lu\n##SBATCH --mail-type=BEGIN,FAIL,END\nmodule load Singularity-CE/3.10.2-GCCcore-11.3.0\n\nIP_ADDRESS=$(hostname -I | awk '{print $1}')\nWEB_PORT=\"8020\"\nVNC_PORT=\"5910\"\nFORWARD_PORT=\"9910\"\necho \"On your laptop: ssh -p 8822 -NL ${FORWARD_PORT}:${IP_ADDRESS}:${WEB_PORT} ${USER}@login.lxp.lu \"\necho \"Open following link http://localhost:${FORWARD_PORT}/vnc.html\"\nsrun -N1 -n1 -c1 singularity exec vnc-rocky8.app /opt/websockify/run --web /opt/noVNC/ ${WEB_PORT} ${IP_ADDRESS}:${VNC_PORT} &amp;\nsrun -N1 -n1 -c1 singularity exec -B /apps:/apps -B /project:/project -B /project:/project -B /mnt:/mnt  vnc-rocky8.app /bin/bash start_script.sh 2&gt;/dev/null\n</code></pre> <ul> <li>Line 12 loads singularity</li> <li>Lines 15-17 defines all ports</li> <li>Lines 18 and 19 just echo the commands you will need to execute locally on your laptop<ul> <li>Line 18: Open a ssh tunnel</li> <li>Line 19: The link to access to the web-based GUI</li> </ul> </li> <li>You can adapt the number of task and the number of cores per tasks</li> </ul> <p>Before using this script, it is more convenient to setup a password in interactive mode. So connect to an interactive job using <code>salloc -A &lt;account&gt; -t 00:30:00 -q default -p cpu -N 1</code>. Load the Singularity module and exec vncpasswd using the container <code>vtune_vnc_rocky8.app</code>. </p> <p>Example</p> <pre><code>$ salloc -A p200117 -t 48:00:00 -q default -p fpga -N 1\n(mel3013)$ module load Singularity-CE/3.10.2-GCCcore-11.3.0\n(mel3013)$ singularity exec vnc-rocky8.app vncpasswd\n           INFO:    Converting SIF file to temporary sandbox...\n           Password:\n</code></pre> <p>Add a password and close the interactive job <code>&lt;CTRL-D&gt;</code>. Your vnc password is now setup.</p> <p>Use <code>sbatch launcher_vnc-rocky8.sh</code> to start the vnc server inside a batch job. Once resources are available, the job starts.</p> <pre><code>$ squeue\nJOBID PARTITION     NAME     USER    ACCOUNT    STATE       TIME   TIME_LIMIT  NODES NODELIST(REASON)\n409979       cpu launcher  u100057    p200117  RUNNING       0:08   1-16:00:00      1 mel0533\n409965      fpga interact  u100057    p200117  RUNNING      47:07   2-00:00:00      1 mel3009\n</code></pre> <ul> <li> <p>Record the jobid and open the slurm output file: <pre><code>cat slurm-409979.out On your laptop: ssh -p 8822 -NL 9910:10.3.25.25:8020 u100057@login.lxp.lu Open following link http://localhost:9910/vnc.html\nINFO:    Converting SIF file to temporary sandbox...\nWebSocket server settings:\n  - Listen on :8020\n  - Web server. Web root: /opt/noVNC\n  - No SSL/TLS support (no cert file)\n- proxying from :8020 to 10.3.25.25:5910\n</code></pre></p> </li> <li> <p>Copy-paste into your laptop terminal the ssh command</p> </li> <li>Then click on the http link. You should see a new tab opening with the following content:</li> </ul> <p></p> <ul> <li>Click on connect and enter your vnc password</li> <li>Your should see now the VTune profiler</li> </ul> <p></p>"},{"location":"optimization/","title":"Optimizing SYCL programs for Intel\u00ae FPGA cards","text":"<p>Optimizing SYCL code for Intel FPGAs requires a combination of understanding the FPGA hardware, the SYCL programming model, and the specific compiler features provided by Intel. Here are some general guidelines to optimize Intel FPGA SYCL code.</p> <p>Compared to OpenCL, the Intel\u00ae oneAPI DPC++ compiler has enhanced features to detect possible optimizations( vectorization, static coalescing, etc ...). Nonetheless, some rules need to be followed to make sure the compiler is able to apply these optimizations. </p> <p>Optimizing your design</p> <p>As this course/workshop is only an introduction to the Intel\u00ae oneAPI for FPGA programming, we can't unfortunately provide all existing and possible optimizations. Many more optimizations can be found in the Intel official documentation.</p>"},{"location":"optimization/#loop-optimization","title":"Loop optimization","text":"<p>Loop unrolling is an optimization technique that aims to increase parallelism and, consequently, the throughput of certain computational tasks, particularly when implemented in hardware environments such as FPGAs. </p> <ol> <li> <p>Pipelining Synergy: Loop unrolling often goes hand in hand with pipelining in FPGAs. When loops are unrolled, each unrolled iteration can be pipelined, leading to even greater throughput enhancements.</p> </li> <li> <p>Resource Utilization: While loop unrolling can significantly speed up operations, it also consumes more FPGA resources, like Logic Elements (LEs) and registers, because of the duplicated hardware. Hence, there's a trade-off between speed and resource utilization.</p> </li> <li> <p>Memory Access: Unrolling loops that involve memory operations can lead to increased memory bandwidth utilization. In cases where memory bandwidth is a bottleneck, unrolling can provide substantial performance improvements.</p> </li> <li> <p>Latency &amp; Throughput: Loop unrolling doesn't necessarily reduce the latency of a single loop iteration (the time taken for one iteration to complete), but it can significantly improve the throughput (number of completed operations per unit time).</p> </li> <li> <p>Reduction in Control Logic: Unrolling can reduce the overhead associated with the loop control logic, such as incrementing the loop counter and checking the loop termination condition.</p> <p> Loop Optimization in HLS </p> </li> <li> <p>Unrolling loops will help to reduce the Initialization Interval (II) as you can notice on the previous figure.</p> </li> </ol> <p>Increasing throughput with loop unrolling</p> How to unroll loopsQuestionSolution <ul> <li>Unrolling loop can be done using the <code>#pragma unroll &lt;N&gt;</code></li> <li><code>&lt;N&gt;</code> is the unroll factor</li> <li><code>#pragma unroll 1</code> : prevent a loop in your kernel from unrolling</li> <li><code>#pragma unroll</code> : let the offline compiler decide how to unroll the loop  <pre><code>handler.single_task&lt;class example&gt;([=]() {\n#pragma unroll\nfor (int i = 0; i &lt; 10; i++) {\nacc_data[i] += i;\n}\n#pragma unroll 1\nfor (int k = 0; k &lt; N; k++) {\n#pragma unroll 5\nfor (int j = 0; j &lt; N; j++) {\nacc_data[j] = j + k;\n}\n}\n});\n</code></pre></li> </ul> <ul> <li>Consider the following code that you can find at <code>oneAPI-samples/DirectProgramming/C++SYCL_FPGA/Tutorials/Features/loop_unroll</code></li> <li>Note that Intel did not consider data alignment which could impact performance</li> <li>We included <code>#include &lt;boost/align/aligned_allocator.hpp&gt;</code> to create aligned std::vector</li> <li>The following SYCL code has been already compiled for you, execute it on the FPGA nodes for several data input size and record the throughput and kernel time</li> <li>What do you observe ? <pre><code>//==============================================================\n// Copyright Intel Corporation\n//\n// SPDX-License-Identifier: MIT\n// =============================================================\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;iomanip&gt;\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n#include &lt;vector&gt;\n#include &lt;boost/align/aligned_allocator.hpp&gt;\nusing namespace sycl;\nusing aligned64_vector= std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt;;\n// Forward declare the kernel name in the global scope.\n// This FPGA best practice reduces name mangling in the optimization reports.\ntemplate &lt;int unroll_factor&gt; class VAdd;\n// This function instantiates the vector add kernel, which contains\n// a loop that adds up the two summand arrays and stores the result\n// into sum. This loop will be unrolled by the specified unroll_factor.\ntemplate &lt;int unroll_factor&gt;\nvoid VecAdd(const aligned64_vector &amp;summands1,\nconst aligned64_vector &amp;summands2, aligned64_vector &amp;sum,\nsize_t array_size) {\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\ntry {\nqueue q(selector,property::queue::enable_profiling{});\nauto device = q.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\nbuffer buffer_summands1(summands1);\nbuffer buffer_summands2(summands2);\nbuffer buffer_sum(sum);\nevent e = q.submit([&amp;](handler &amp;h) {\naccessor acc_summands1(buffer_summands1, h, read_only);\naccessor acc_summands2(buffer_summands2, h, read_only);\naccessor acc_sum(buffer_sum, h, write_only, no_init);\nh.single_task&lt;VAdd&lt;unroll_factor&gt;&gt;([=]()\n[[intel::kernel_args_restrict]] {\n// Unroll the loop fully or partially, depending on unroll_factor\n#pragma unroll unroll_factor\nfor (size_t i = 0; i &lt; array_size; i++) {\nacc_sum[i] = acc_summands1[i] + acc_summands2[i];\n}\n});\n});\ndouble start = e.get_profiling_info&lt;info::event_profiling::command_start&gt;();\ndouble end = e.get_profiling_info&lt;info::event_profiling::command_end&gt;();\n// convert from nanoseconds to ms\ndouble kernel_time = (double)(end - start) * 1e-6;\nstd::cout &lt;&lt; \"unroll_factor \" &lt;&lt; unroll_factor\n&lt;&lt; \" kernel time : \" &lt;&lt; kernel_time &lt;&lt; \" ms\\n\";\nstd::cout &lt;&lt; \"Throughput for kernel with unroll_factor \" &lt;&lt; unroll_factor\n&lt;&lt; \": \";\nstd::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(3)\n#if defined(FPGA_SIMULATOR)\n&lt;&lt; ((double)array_size / kernel_time) / 1e3f &lt;&lt; \" MFlops\\n\";\n#else\n&lt;&lt; ((double)array_size / kernel_time) / 1e6f &lt;&lt; \" GFlops\\n\";\n#endif\n} catch (sycl::exception const &amp;e) {\n// Catches exceptions in the host code\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\n}\nint main(int argc, char *argv[]) {\n#if defined(FPGA_SIMULATOR)\nsize_t array_size = 1 &lt;&lt; 4;\n#else\nsize_t array_size = 1 &lt;&lt; 26;\n#endif\nif (argc &gt; 1) {\nstd::string option(argv[1]);\nif (option == \"-h\" || option == \"--help\") {\nstd::cout &lt;&lt; \"Usage: \\n&lt;executable&gt; &lt;data size&gt;\\n\\nFAILED\\n\";\nreturn 1;\n} else {\narray_size = std::stoi(option);\n}\n}\naligned64_vector summands1(array_size);\naligned64_vector summands2(array_size);\naligned64_vector sum_unrollx1(array_size);\naligned64_vector sum_unrollx2(array_size);\naligned64_vector sum_unrollx4(array_size);\naligned64_vector sum_unrollx8(array_size);\naligned64_vector sum_unrollx16(array_size);\n// Initialize the two summand arrays (arrays to be added to each other) to\n// 1:N and N:1, so that the sum of all elements is N + 1\nfor (size_t i = 0; i &lt; array_size; i++) {\nsummands1[i] = static_cast&lt;float&gt;(i + 1);\nsummands2[i] = static_cast&lt;float&gt;(array_size - i);\n}\nstd::cout &lt;&lt; \"Input Array Size:  \" &lt;&lt; array_size &lt;&lt; \"\\n\";\n// Instantiate VecAdd kernel with different unroll factors: 1, 2, 4, 8, 16\n// The VecAdd kernel contains a loop that adds up the two summand arrays.\n// This loop will be unrolled by the specified unroll factor.\n// The sum array is expected to be identical, regardless of the unroll factor.\nVecAdd&lt;1&gt;(summands1, summands2, sum_unrollx1, array_size);\nVecAdd&lt;2&gt;(summands1, summands2, sum_unrollx2, array_size);\nVecAdd&lt;4&gt;(summands1, summands2, sum_unrollx4, array_size);\nVecAdd&lt;8&gt;(summands1, summands2, sum_unrollx8, array_size);\nVecAdd&lt;16&gt;(summands1, summands2, sum_unrollx16, array_size);\n// Verify that the output data is the same for every unroll factor\nfor (size_t i = 0; i &lt; array_size; i++) {\nif (sum_unrollx1[i] != summands1[i] + summands2[i] ||\nsum_unrollx1[i] != sum_unrollx2[i] ||\nsum_unrollx1[i] != sum_unrollx4[i] ||\nsum_unrollx1[i] != sum_unrollx8[i] ||\nsum_unrollx1[i] != sum_unrollx16[i]) {\nstd::cout &lt;&lt; \"FAILED: The results are incorrect\\n\";\nreturn 1;\n}\n}\nstd::cout &lt;&lt; \"PASSED: The results are correct\\n\";\nreturn 0;\n}\n</code></pre></li> </ul> Unroll factor kernel execution time (ms) Throughput (GFlops) 1 77 0.447 2 58 0.591 4 43 0.804 8 40 0.857 16 39 0.882 <ul> <li>Increasing the unroll factor improves throughput    </li> <li>Nonetheless, unrolling large loops should be avoided as it would require a large amount of hardware</li> </ul> <p>Recording kernel time</p> <ul> <li>In this example, we have also seen how to record kernel time.</li> <li>Using the property `property::queue::enable_profiling{}`` adds the requirement that the runtime must capture profiling information for the command groups that are submitted from the queue </li> <li>You can the capture  the start &amp; end time using the following two commands:<ul> <li><code>double start = e.get_profiling_info&lt;info::event_profiling::command_start&gt;();</code></li> <li><code>double end = e.get_profiling_info&lt;info::event_profiling::command_end&gt;();</code></li> </ul> </li> </ul> <p>Caution with nested loops</p> <ul> <li>Loop unrolling involves replicating the hardware of a loop body multiple times and reducing the trip count of a loop. Unroll loops to reduce or eliminate loop control overhead on the FPGA. </li> <li>Loop-unrolling can be used to eliminate nested-loop structures.</li> <li>However avoid unrolling the outer-loop which will lead to Resource Exhaustion and dramatically increase offline compilation</li> </ul>"},{"location":"optimization/#simd-work-items-for-nd-range-kernels","title":"SIMD Work Items for ND-Range kernels","text":"<ul> <li> <p>In the Reporting &amp; Profiling section we have seen that vectorization can improve bandwidth</p> </li> <li> <p>ND-range kernel should use instead of classical data-parallel kernels</p> </li> <li>The work-group size needs to be set using the attribute <code>[[sycl::reqd_work_group_size(1, 1, REQD_WG_SIZE)]]</code></li> <li>To specify the number of SIMD work_items, you will need to add the following attribute <code>[[intel::num_simd_work_items(NUM_SIMD_WORK_ITEMS)]]</code></li> <li>Note that NUM_SIMD_WORK_ITEMS should divide evenly REQD_WG_SIZE</li> <li>The supported values for NUM_SIMD_WORK_ITEMS  are 2, 4, 8, and 16</li> </ul> <p>Example<pre><code>...\nh.parallel_for&lt;VectorAddID&gt;(\nsycl::nd_range&lt;1&gt;(sycl::range&lt;1&gt;(2048), sycl::range&lt;1&gt;(128)),        [=](sycl::nd_item&lt;1&gt; it) [[intel::num_simd_work_items(8),\nsycl::reqd_work_group_size(1, 1, 128)]] {\nauto gid = it.get_global_id(0);\naccessor_c[gid] = accessor_a[gid] + accessor_b[gid];\n});\n});\n...\n</code></pre> <ul> <li>The 128 work-items are evenly distributed among 8 SIMD lanes</li> <li>\\frac{128}{8} = 16 wide vector operation</li> <li>The offline compiler coalesces 8 loads to optimize (reduce) the access to memory in case there are no data dependencies</li> </ul> </p>"},{"location":"optimization/#loop-coalescing","title":"Loop coalescing","text":"<p>Utilize the <code>loop_coalesce</code> attribute to instruct the Intel\u00ae oneAPI DPC++/C++ Compiler to merge nested loops into one, preserving the loop's original functionality. By coalescing loops, you can minimize the kernel's area consumption by guiding the compiler to lessen the overhead associated with loop management.</p> <p>Coalesced two loops</p> Using the loop_coalesce attribute <pre><code>[[intel::loop_coalesce(2)]]\nfor (int i = 0; i &lt; N; i++)\nfor (int j = 0; j &lt; M; j++)\nsum[i][j] += i+j;\n</code></pre> Equivalent code <pre><code>int i = 0;\nint j = 0;\nwhile(i &lt; N){\nsum[i][j] += i+j;\nj++;\nif (j == M){\nj = 0;\ni++;\n}\n}\n</code></pre>"},{"location":"optimization/#ignore-loop-carried-dependencies","title":"Ignore Loop-carried dependencies","text":"<p>The ivdep attribute in Intel's oneAPI (as well as in other Intel compiler tools) is used to give a hint to the compiler about the independence of iterations in a loop. This hint suggests that there are no loop-carried memory dependencies that the compiler needs to account for when attempting to vectorize or parallelize the loop.</p> <p>When you use ivdep, you're essentially telling the compiler: \"Trust me, I've reviewed the code, and the iterations of this loop do not have dependencies on each other. So, you can safely vectorize or parallelize this loop for better performance.\"</p> <p>ivdep attribute</p> <pre><code>#pragma ivdep\nfor (int i = 1; i &lt; N; i++) {\nA[i] = A[i - 1] + B[i];\n}\n</code></pre> <p>Caution</p> <p>You should be very careful when using ivdep. Incorrectly using this pragma on a loop that does have dependencies can lead to unexpected results or undefined behavior. Always ensure that there are truly no dependencies in the loop before applying this hint.</p>"},{"location":"optimization/#memory","title":"Memory","text":""},{"location":"optimization/#static-coalescing","title":"Static coalescing","text":"<ul> <li> <p>Static coalescing is performed by the Intel\u00ae oneAPI DPC++/C++ Compiler contiguous accesses to global memory can be merged into a single wide access.</p> </li> <li> <p>For static memory coalescing to occur, your code should be structured so that the compiler can detect a linear access pattern at compile time. The initial kernel code depicted in the previous figure can leverage static memory coalescing, as all indices into buffers a and b increase with offsets recognizable during compilation.</p> </li> </ul> <p> </p> FPGA Optimization Guide for Intel\u00ae oneAPI Toolkits -- Figure 17-21"},{"location":"optimization/#data-structure-alignment","title":"Data structure alignment","text":"<p>In order to performance, structure alignment can be modified to be properly aligned. By default, the offline compiler aligns these elements based on:</p> <ul> <li>The alignment should be a power of two.</li> <li>The alignment should be a multiple of the least common multiple (LCM) of the word-widths of the structure member sizes.</li> </ul> <p>Let's take a simple but clear example to understand why alignment is so important.</p> <p></p> <p>Removing padding and changing structure alignment</p> CodeExecution time <ul> <li> <p>The following code show the impact of changing the alignmement and padding using three scenarii:</p> <ul> <li> <p>Default alignment and padding </p> </li> <li> <p>Removing padding</p> </li> <li> <p>Changing alignment </p> </li> </ul> </li> </ul> <pre><code>#include &lt;iostream&gt;\n#include &lt;typeinfo&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;chrono&gt;\nusing namespace std::chrono;\n#define ALIGNMENT 64\n#define IT 1024\nconstexpr int kVectSize = 2048;\ntemplate&lt;typename T&gt;\nvoid test_structure( T* device,sycl::queue &amp;q, int nb_iters){\nsycl::event e;\nconst sycl::property_list props = {sycl::property::buffer::use_host_ptr()};\nauto start = high_resolution_clock::now();\nsycl::buffer buffer_device{device, sycl::range(kVectSize),props};\ne = q.submit([&amp;](sycl::handler &amp;h) {\nsycl::accessor accessor_device{buffer_device, h, sycl::read_write};\nh.single_task([=]() {\nfor(int it=0;it &lt; nb_iters ;it++){\nfor (int idx = 0; idx &lt; kVectSize; idx++) {\naccessor_device[idx].C = (int)accessor_device[idx].A + accessor_device[idx].B;\n}\n}\n});\n});\nsycl::host_accessor buffer_host(buffer_device);\nauto stop = high_resolution_clock::now();\n// convert from nanoseconds to ms\nduration&lt;double&gt; kernel_time = stop - start;\nstd::cout  &lt;&lt; \" Time (\" &lt;&lt;typeid(T).name()&lt;&lt;  \") : \" &lt;&lt; kernel_time.count() &lt;&lt; \" ms\\n\";\n}\nint main() {\nbool passed = true;\ntry {\n// Use compile-time macros to select either:\n//  - the FPGA emulator device (CPU emulation of the FPGA)\n//  - the FPGA device (a real FPGA)\n//  - the simulator device\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// create the device queue\nsycl::queue q(selector,sycl::property::queue::enable_profiling{});\n// make sure the device supports USM host allocations\nauto device = q.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\n// declare arrays and fill them\ntypedef struct {\nchar A;\nint  B;\nint  C;\n} mystruct;\ntypedef struct __attribute__ ((packed)) {\nchar A;\nint  B;\nint  C;\n} mystruct_packed;\ntypedef struct __attribute__ ((packed)) __attribute__ ((aligned(16))) {\nchar A;\nint  B;\nint  C;\n} mystruct_packed_aligned;\n//mystruct host_vec_a[kVectSize];\n//mystruct_packed host_vec_b[kVectSize];\n//mystruct_packed_aligned host_vec_c[kVectSize];\nmystruct* vec_a = new(std::align_val_t{ 64 }) mystruct[kVectSize];\nmystruct_packed* vec_b = new(std::align_val_t{ 64 }) mystruct_packed[kVectSize];\nmystruct_packed_aligned* vec_c = new(std::align_val_t{ 64 }) mystruct_packed_aligned[kVectSize];\n//mystruct * vec_a = static_cast&lt;mystruct*&gt;(aligned_alloc_device(ALIGNMENT,kVectSize*sizeof(mystruct),q));\n//mystruct_packed*vec_b = static_cast&lt;mystruct_packed*&gt;(aligned_alloc_device(ALIGNMENT,kVectSize*sizeof(mystruct_packed),q));\n//mystruct_packed_aligned*vec_c = static_cast&lt;mystruct_packed_aligned*&gt;(aligned_alloc_device(ALIGNMENT,kVectSize*sizeof(mystruct_packed_aligned),q));\nfor (int i = 0; i &lt; kVectSize; i++) {\nvec_a[i].A = vec_b[i].A = vec_c[i].A = char(std::rand() % 256);\nvec_a[i].B = vec_b[i].B = vec_c[i].B = std::rand();\nvec_a[i].C = vec_b[i].C = vec_c[i].C = std::rand();\n}\nstd::cout &lt;&lt; \"Packed with default alignment\" &lt;&lt; kVectSize &lt;&lt; std::endl;\ntest_structure&lt;mystruct&gt;(vec_a,q,IT);\ntest_structure&lt;mystruct_packed&gt;(vec_b,q,IT);\ntest_structure&lt;mystruct_packed_aligned&gt;(vec_c,q,IT);\ndelete[] vec_a;\ndelete[] vec_b;\ndelete[] vec_c;\n//sycl::free(vec_a,q);\n//sycl::free(vec_b,q);\n//sycl::free(vec_c,q);\n} catch (sycl::exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> Scenario Processing time (seconds) Default alignment and padding 14.33 Removing padding 6.35 Changing alignment 0.03"},{"location":"optimization/#local-memory","title":"Local memory","text":""},{"location":"optimization/#local-memory-in-nd-range-kernels","title":"Local memory in ND-Range kernels","text":"<ul> <li>You can improve memory access by using local and private memory.</li> <li>When you define a private array, group local memory, or a local accessor, the Intel\u00ae oneAPI DPC++/C++ Compiler generates kernel memory in the hardware. This kernel memory is often termed on-chip memory since it originates from memory resources, like RAM blocks, present on the FPGA.</li> <li>Local or private memory is a fast memory that should be favored when resources allow.</li> </ul> <p>Private memory</p> <pre><code>...\nq.submit([&amp;](handler &amp;h) {\n// Create an accessor for device global memory from buffer buff\naccessor acc(buff, h, write_only);\ncgh.single_task([=]() {\n// Declare a private array\nint T[N];\n// Write to private memory\nfor (int i = 0; i &lt; N; i++)\nT[i] = i;\n// Read from private memory and write to global memory through the accessor\nfor (int i = 0; i &lt; N; i+=2)\nacc[i] = T[i] + T[i+1];\n});\n}); ...\n</code></pre> <ul> <li>To set aside local memory that can be accessed and shared by every work item within a workgroup, establish a group-local variable within the function scope of a workgroup. Do this using the group_local_memory_for_overwrite function, illustrated in the subsequent example:</li> </ul> <p>Local memory</p> <pre><code>...\nq.submit([&amp;](handler &amp;h) {\nh.parallel_for(\nnd_range&lt;1&gt;(range&lt;1&gt;(256), range&lt;1&gt;(16)), [=](nd_item&lt;1&gt; item) {\nint local_id = item.get_local_id();\nauto ptr = group_local_memory_for_overwrite&lt;int[16]&gt;(item.get_group());\nauto&amp; ref = *ptr;\nref[local_id] = local_id++ ;\n});\n});\n... </code></pre> <ul> <li>The ND-Range kernel has 16 workgroups with 16 work items for each group.</li> <li>A group-local variable (int[16]) is created for each group and shared through a multi_ptr to all work-items of the same group</li> </ul>"},{"location":"optimization/#settings-memory-banks","title":"Settings memory banks","text":"<ul> <li>Local data can be stored  in separate  local memory banks for parallel memory accesses</li> <li>Number of banks of a local memory can be adjusted (e.g., to increase the parallel access) </li> <li>Add the following attributes <code>[[intel::numbanks(#NB), intel::bankwidth(#BW)]]</code>:  <ul> <li><code>#NB</code> : number of banks </li> <li><code>#BW</code>: bankwidth to be considered </li> </ul> </li> <li>Ex: <code>[[intel::numbanks(8), intel::bankwidth(16)]]lmem[8][4]</code>; <ul> <li>No two element can be accessed in parallel in lmem </li> <li>Single bank local memory </li> </ul> </li> <li>All rows accessible in parallel with numbanks(8) </li> <li>Different configurations patterns can be adopted </li> </ul> <p>Masking the last index</p> <ul> <li>Intel's documentation states that \"To enable parallel access, you must mask the dynamic access on the lower array index\" <pre><code>[[intel::numbanks(8), intel::bankwidth(16)]] int lmem[8][4];\n#pragma unroll\nfor (int i = 0; i &lt; 4; i+=2) {\nlmem[i][x &amp; 0x3] = ...;\n} </code></pre></li> </ul> <p>Exercice</p> QuestionSolution <ul> <li>Could you briefly describe the bank configuration of the following local memory declaration; <pre><code> [[intel::numbanks(4),intel::bankwidth(8)]] int lmem[2][4];\n</code></pre></li> </ul> <p></p>"},{"location":"optimization/#local-memory-replication","title":"Local memory replication","text":"<p>Example</p> <p> <pre><code>[[intel::fpga_memory,\nintel::singlepump,\nintel::max_replicates(3)]] int lmem[16]; lmem[waddr] = lmem[raddr] +\nlmem[raddr + 1] +\nlmem[raddr + 2]; </code></pre> <ul> <li>The offline compiler can replicate the local memory</li> <li>This allows to create multiple ports </li> <li>Behaviour: <ul> <li>All read ports will be accessed in parallel </li> <li>All write ports are connected together</li> <li>Data between replicate is identical </li> </ul> </li> <li>Parallel access to all ports is possible but consumes more hardware resources</li> <li><code>[[intel::max_replicates(N)]]</code> control the replication factor</li> </ul> <p> </p>"},{"location":"optimization/#task-parallelism-with-inter-kernel-pipes","title":"Task parallelism with Inter-Kernel Pipes","text":"<p>Pipes function as a first-come, first-served buffer system, linking different parts of a design. The Intel\u00ae oneAPI DPC++/C++ Compiler offers various pipe types:</p> <ul> <li> <p>Host Pipes: These establish a connection between a host and a device.</p> </li> <li> <p>Inter-Kernel Pipes: These facilitate efficient and low-latency data transfer and synchronization between kernels. They enable kernels to interact directly using on-device FIFO buffers, which utilize FPGA memory. The Intel\u00ae oneAPI DPC++/C++ Compiler promotes simultaneous kernel operation. By employing inter-kernel pipes for data movement among concurrently running kernels, data can be transferred without waiting for a kernel to finish, enhancing your design's throughput.</p> </li> <li> <p>I/O Pipes: This is a one-way connection to the hardware, either as a source or sink, which can be linked to an FPGA board's input or output functionalities. Such functionalities could encompass network interfaces, PCIe\u00ae, cameras, or other data acquisition or processing tools and protocols.</p> </li> </ul>"},{"location":"optimization/#inter-kernel-pipes","title":"Inter-Kernel Pipes","text":"<ul> <li>We will only focus on Inter-Kernel Pipes to leverage task parallelism</li> <li>As for OpenCL programming, pipes can be blocking or non-blocking</li> <li>For Intel\u00ae oneAPI with FPGA, you need to include FPGA extension: <pre><code>#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n</code></pre></li> </ul> <p>Pipe creation and usage</p> Blocking pipesNon-Blocking pipes <pre><code>// Using alias eases considerably their usage\nusing my_pipe = ext::intel::pipe&lt;      class InterKernelPipe, // An identifier for the pipe.\nint,                   // The type of data in the pipe.\n4&gt;;                    // The capacity of the pipe.\n// Single_task kernel 1\nq.submit([&amp;](handler&amp; h) {\nauto A = accessor(B_in, h);\nh.single_task([=]() {\nfor (int i=0; i &lt; count; i++) {\nmy_pipe::write(A[i]); // write a single int into the pipe\n}\n});\n}); // Single_task kernel 2\nq.submit([&amp;](handler&amp; h) {\nauto A = accessor(B_out, h);\nh.single_task([=]() {\nfor (int i=0; i &lt; count; i++) {\nA[i] = my_pipe::read(); // read the next int from the pipe\n}\n});\n}); </code></pre> <pre><code>// Using alias eases considerably their usage\nusing my_pipe = ext::intel::pipe&lt;      class InterKernelPipe, // An identifier for the pipe.\nint,                   // The type of data in the pipe.\n4&gt;;                    // The capacity of the pipe.\n// Single_task kernel 1\nq.submit([&amp;](handler&amp; h) {\nauto A = accessor(B_in, h);\nh.single_task([=]() {\nvalid_write = false;\nfor (int i=0; i &lt; count; i++) {\nmy_pipe::write(A[i],valid_write); // write a single int into the pipe\n}\n});\n}); // Single_task kernel 2\nq.submit([&amp;](handler&amp; h) {\nauto A = accessor(B_out, h);\nh.single_task([=]() {\nvalid_read = false;\nfor (int i=0; i &lt; count; i++) {\nA[i] = my_pipe::read(valid_read); // read the next int from the pipe\n}\n});\n}); </code></pre> <p>Stalling pipes</p> <ul> <li>Care should be taken when implementing pipes, especially when there is a strong imbalance between the consumer kernel reading from the pipe and the producer kernel that feed the pipe. </li> <li>Stalling pipes can be disastrous when using blocking pipes</li> </ul>"},{"location":"reporting_profiling/","title":"Reporting &amp; Profiling SYCL programs for Intel\u00ae FPGA cards","text":"<p>After having spent some time to write your kernel and debug functional problems, it's now time to take advantage of the accelerator. FPGA uses pipelining parallelism but how does it work ? </p> <p></p>"},{"location":"reporting_profiling/#definitions","title":"Definitions","text":"<p>Pipelining (see FPGA Optimization Guide for Intel\u00ae oneAPI Toolkits)</p> <p>Pipelining is a design technique used in synchronous digital circuits to increase fMAX. Pipelining involves adding registers to the critical path, which decreases the amount of logic between each register. Less logic takes less time to execute, which enables an increase in f MAX. The critical path in a circuit is the path between any two consecutive registers with the highest latency. That is, the path between two consecutive registers where the operations take the longest to complete. Pipelining is especially useful when processing a stream of data. A pipelined circuit can have different stages of the pipeline operating on different input stream data in the same clock cycle, which leads to better data processing throughput. </p> <p>Maximum Frequency (fMAX)</p> <p>The fMAX of a digital circuit is its highest possible clock frequency, determining the maximum rate for updating register outputs. This speed is constrained by the physical propagation delay of the signal across the combinational logic between consecutive register stages. The delay is affected by the complexity of the combinational logic in the path, and the path with the greatest number of logic elements and highest delay sets the speed limit for the entire circuit, often known as the critical path. The fMAX is the reciprocal of this critical path delay, and having a high fMAX is desirable as it leads to better performance when there are no other restrictions. </p> <p>Throughput</p> <p>Throughput in a digital circuit refers to the speed at which data is handled. When there are no other limiting factors, a higher fMAX leads to increased throughput, such as more samples per second. Often synonymous with performance, throughput is frequently used to gauge the effectiveness of a circuit. </p> <p>Latency</p> <p>Latency measures the duration to complete operations in a digital circuit, and it can be assessed for individual tasks or the whole circuit. It can be measured in time units like microseconds or clock cycles, with the latter often preferred. Measuring latency in clock cycles separates it from the circuit's clock frequency, making it easier to understand the real effects of modifications on the circuit's performance. </p> <p>Occupancy</p> <p>The occupancy of a datapath at a specific moment signifies the fraction of the datapath filled with valid data. When looking at a circuit's execution of a program, the occupancy is the mean value from the beginning to the end of the program's run. Parts of the datapath that are unoccupied are commonly called bubbles, akin to a CPU's no-operation (no-ops) instructions, which don't influence the final output. Minimizing these bubbles leads to greater occupancy. If there are no other hindrances, optimizing the occupancy of the datapath will boost throughput.  Occupancy: \\frac{2}{5}=40\\% </p>"},{"location":"reporting_profiling/#reporting","title":"Reporting","text":"<p>If you instruct the DPC++ compiler to stop compiling the design after generating the early image, you will be able to access precious information including performance and area estimates without having to wait many hours. The FPGA Early Image can be analyzed using the FPGA Optimization Report to provide:</p> <ul> <li>loop analysis</li> <li>area estimates</li> <li>kernel memory information </li> <li> <p>scheduler information</p> </li> <li> <p>Recall that the FPGA Early image can be obtained using the command: <code>icpx -fsycl -fintelfpga -qactypes -Xshardware -fsycl-link=early -Xstarget=Stratix10 accumulator.cpp -o accumulator_report.a</code></p> </li> <li> <p>You can evaluate whether the estimated kernel performance data is satisfactory by going to the /reports/ directory and examining one of the following files related to your application: <li> <p>report.html: This file can be viewed using Internet browsers of your choice</p> </li> <li>.zip: Utilize the Intel\u00ae oneAPI FPGA Reports tool,i.e., <code>fpga_report</code> <p>Analyzing the FPGA Early Image report</p> SetupQuestionSolution <ul> <li>First,copy <code>/project/home/p200117/FPGA/05-accumulator</code> to  your home folder    </li> <li>Generate you the early image with the report using:  <pre><code>  # Don't forget to be on a node first\nicpx -fsycl -fintelfpga -qactypes -Xshardware -fsycl-link=early -Xstarget=Stratix10 accumulator.cpp -o accumulator_report.a`\n</code></pre></li> <li>The next step is to follow the Graphical sessions guide</li> <li>Once connected to the vnc session, you should see something like this:  </li> <li>Open a terminal using the GUI interface and got to <code>05-accumulator/src/accumulator_report.prj/reports/</code> directory</li> <li>Open the file <code>report.html</code> with firefox  </li> </ul> <ul> <li>Check the loop analysis report. What do you observe ?</li> <li>What is the predicted fMAX ?</li> <li>What is the block scheduled II ? What is its impact ? </li> </ul> <ul> <li>We have a data-dependency at line 59</li> <li>For each loop iteration, the Intel\u00ae oneAPI DPC++/C++ Compiler takes 3 cycles to compute the result of the addition and then stores it in the variable temp_sum</li> <li>We either need to remove the data dependency or to relax it</li> </ul> <p>Analyzing the final FPGA Image report</p> QuestionSolution <ul> <li>Open <code>/project/home/p200117/FPGA/06-shift_register/src/shift_register.fpga.prj/reports/report.html</code></li> <li>Did the shift register solve the problem ? What is now the Initialization Interval ?</li> </ul> <ul> <li>We can force the II using <code>#pragma II &lt;N&gt;</code> but it will trigger an error if it can reduced it</li> <li>The shift register is a very efficient design pattern for FPGA programming as it increase the dependence distance between loops.</li> <li>Relaxation means increasing this distance</li> </ul>"},{"location":"reporting_profiling/#profiling","title":"Profiling","text":"<ul> <li>The Intel\u00ae FPGA dynamic profiler for DPC++ can be used to add performance counters to the design and collect performance data during execution. This requires full hardware compilation.</li> <li>In order to instruct the offline compiler to add those performance counters, one needs to add the compilation option <code>-Xsprofile</code>.</li> </ul> Intel\u00ae FPGA Dynamic Profiler for DPC++: Performance Counters Instrumentation <p>Performance counters are attached to every load and store instruction, and they are linked together in a sequence that connects to the Control Register Access (CRA). The CRA interface provides access to the control and status registers interface.</p> <ul> <li>Once the design has been compiled, performance data can be obtained at runtime:<ul> <li>Either using your host application in the Intel\u00ae VTuneTM Profiler with CPU/FPGA Interaction view</li> <li>Or using the command line by using the Profiler Runtime Wrapper. Data can later be importer to the  Intel\u00ae VTuneTM Profiler</li> </ul> </li> </ul>"},{"location":"reporting_profiling/#profiler-runtime-wrapper","title":"Profiler Runtime Wrapper","text":"<ul> <li>You need to execute the FPGA executable using the Profiler Runtime Wrapper to fill the profiling results using the following command: <pre><code>aocl profile [options] /path/to/executable [executable options]\n</code></pre></li> <li>The Profiler Runtime Wrapper calls your executable and collects profile information</li> <li>The performance counter data is saved in a <code>profile.mon</code> monitor description file that the Profiler Runtime Wrapper post-processes and outputs into a readable <code>profile.json</code> file</li> <li> <p>Intel recommends the use of the <code>profile.json</code> for further data processing</p> </li> <li> <p>Note that you can control the sample rate used by the Profiler Runtime Wrapper using the <code>-period &lt;N&gt;</code> option</p> </li> <li> <p>Use the command <code>aocl profile -help</code> to get more details: <pre><code> aocl profile --help\n   aocl profile can be used to collect information about your host run if you compiled with -profile. \n   To use it, please specify your host executable as follows: 'aocl profile path/to/executable'. \n\n   If you are compiling with the oneAPI Data Parallel C++ Compiler and do not wish to pass in the \n   compiled executable binary directly (but rather a script that calls it), the binary needs to be \n   passed in using '--executable' or '-e'. \n\n   It is also optional (but recommended) that you include the location of the *.aocx file \n   using '--aocx' or '-x'. Note that this file is not needed when compiling with the \n   oneAPI Data Parallel C++ Compiler. If no files are given, any aocx found \n   in the current directory will be used (potentially incorrectly) \n\n   OpenCL use case example: aocl profile -x kernel.aocx bin/host -host-arg1 -host-arg2 \n\n   oneAPI use case example: aocl profile -e kernel.fpga executable_calling_kernel_fpga.sh -executable-arg1 \n\n   You can also specify a few other options (after the executable if relevant): \n     - Adjust the period between counter readbacks by specifying the number of clock cycles between subsequent\n       readbacks with '-period ###': 'aocl profile path/to/executable -period 10000' \n       If this flag is not passed in, the counters will be read back as often as possible. \n     - Change counters to only read when the kernel finishes (not while it's running) with -no-temporal \n     - Turn off memory transfer information with -no-mem-transfers \n     - Turn on shared counter data (use when design compiled with '-profile-shared-counters' option) \n     - Change the output directory where the .mon and .json file will be placed with '-output-dir /path/to/new/loc/' \n     - Skip the actual compile and just point to a profile.mon file with '-no-run /path/to/profile.mon' \n       Do this if you already have data, but want it in a format that VTune can display. \n     - Do not create a profile.json file by setting the '-no-json' flag (no need for .aocx or .source files) \n       Do this if you do not wish to visualize the profiler data with VTune, and want the profile.mon output\n\n   Please ensure that the executable and its options are the last arguments. \n</code></pre></p> </li> </ul>"},{"location":"reporting_profiling/#performance-data","title":"Performance data","text":"Stall (%)Occupancy (%)Bandwidth <p>The percentage of time that memory or pipe access leads to pipeline stalls represents a measure of the efficiency of the memory or pipe in fulfilling access requests. It reflects how often these components can successfully meet a request without causing a delay or interruption in the processing flow.</p> <p>the proportion of the total time profiled in which a valid work-item is executing a memory or pipe instruction. It quantifies the fraction of the observed period when actual processing tasks related to memory or pipe instructions are being carried out.</p> <p>The average memory bandwidth refers to the efficiency of memory access in an FPGA. For each global memory access, resources are assigned to obtain data, but the kernel program might use less than the acquired amount. The overall efficiency is the percentage of the total bytes retrieved from the global memory system that the kernel program actually utilizes.</p>"},{"location":"reporting_profiling/#example","title":"Example","text":"<p>Improve Bandwith using vectorization</p> Without SIMDHow to use vectorizationWith SIMD <ul> <li>The folder <code>/project/home/p200117/FPGA/07-vector_add_ndrange_profiling/src</code> contains  FPGA image compiled with the option <code>-Xsprofile</code></li> <li>The kernel use a simple data-parallel kernel (no work-groups) to sum two arrays (size=2048) </li> <li>Using the GUI interface used previously, open the VTune software  </li> <li>Create a new project and import <code>/project/home/p200117/FPGA/07-vector_add_ndrange_profiling/src/profiling</code> which contains the files <code>profile.json</code> and <code>profile.mon</code> </li> <li>Once loaded, open the \"Bottom-up\" tab </li> <li>We have a high occupancy and a average bandwidth of 4.5 GB/s far from the theoretical bandwidth of 12.8 GB/s for a single pseudo-channel </li> </ul> <ul> <li>You will first need to use a ND-range kernel and define your work-group size using the attribute [[sycl::reqd_work_group_size(1, 1, REQD_WG_SIZE)]]</li> <li>To specify the number of SIMD work_items, you will need to add the following attribute [[intel::num_simd_work_items(NUM_SIMD_WORK_ITEMS)]] with     NUM_SIMD_WORK_ITEMS dividing evenly REQD_WG_SIZE</li> <li>The supported values for NUM_SIMD_WORK_ITEMS  are 2, 4, 8, and 16</li> <li>Example <pre><code>...\nh.parallel_for&lt;VectorAddID&gt;(\nsycl::nd_range&lt;1&gt;(sycl::range&lt;1&gt;(2048), sycl::range&lt;1&gt;(128)),        [=](sycl::nd_item&lt;1&gt; it) [[intel::num_simd_work_items(8),\nsycl::reqd_work_group_size(1, 1, 128)]] {\nauto gid = it.get_global_id(0);\naccessor_c[gid] = accessor_a[gid] + accessor_b[gid];\n});\n});\n...\n</code></pre></li> <li>The 128 work-items are evenly distributed among 8 SIMD lanes</li> <li>\\frac{128}{8} = 16 wide vector operation</li> <li>The offline compiler coalesces 8 loads to optimize (reduce) the access to memory in case there are no data dependencies</li> </ul> <ul> <li>The folder <code>/project/home/p200117/FPGA/08-vector_add_ndrange_profiling_simd/src</code> contains  the vectorized version</li> <li>Load into the current project the <code>/project/home/p200117/FPGA/07-vector_add_ndrange_profiling/src/profiling</code> folder which contains the files <code>profile.json</code> and <code>profile.mon</code> </li> <li>The bandwidth is now 9.6 GB/s</li> </ul>"},{"location":"tmux_screen/","title":"Persistent terminal sessions","text":""},{"location":"tmux_screen/#persistent-terminal-sessions-using-gnu-screen","title":"Persistent Terminal Sessions using GNU Screen","text":"<p>GNU Screen is a tool to manage persistent terminal sessions. It becomes interesting since you will probably end at some moment with the following  scenario:</p> <p>you frequently program and run computations on the UL HPC platform i.e on a remote Linux/Unix computer, typically working in six different terminal logins to the access server from your office workstation, cranking up long-running computations that are still not finished and are outputting important information (calculation status or results), when you have 2 interactive jobs running... But it's time to catch the bus and/or the train to go back home.</p> <p>Probably what you do in the above scenario is to</p> <p>a. clear and shutdown all running terminal sessions</p> <p>b. once at home when the kids are in bed, you're logging in again... And have to set up the whole environment again (six logins, 2 interactive jobs etc. )</p> <p>c. repeat the following morning when you come back to the office.</p> <p>Enter the long-existing and very simple, but totally indispensable GNU screen command. It has the ability to completely detach running processes from one terminal and reattach it intact (later) from a different terminal login.</p> <p>Note that screen is not available anymore on modern system, especially when using the Aion cluster, you should use Tmux instead.</p>"},{"location":"tmux_screen/#pre-requisite-screen-configuration-file-screenrc","title":"Pre-requisite: screen configuration file <code>~/.screenrc</code>","text":"<p>While not mandatory, we advise you to rely on our customized configuration file for screen <code>.screenrc</code> available on Github.</p> <p>Otherwise, simply clone the ULHPC dotfile repository and make a symbolic link <code>~/.screenrc</code> targeting the file <code>screen/screenrc</code> of the repository.</p>"},{"location":"tmux_screen/#screen-commands","title":"Screen commands","text":"<p>You can start a screen session (i.e. creates a single window with a shell in it) with the <code>screen</code> command. Its main command-lines options are listed below:</p> <ul> <li><code>screen</code>: start a new screen</li> <li><code>screen -ls</code>: does not start screen, but prints a list of <code>pid.tty.host</code> strings identifying your current screen sessions.</li> <li><code>screen -r</code>: resumes a detached screen session</li> <li><code>screen -x</code>: attach to a not detached screen session. (Multi display mode i.e. when you and another user are trying to access the same session at the same time)</li> </ul> <p>Once within a screen, you can invoke a screen command which consist of a \"<code>CTRL + a</code>\" sequence followed by one other character. The main commands are:</p> <ul> <li><code>CTRL + a c</code>: (create) creates a new Screen window. The default Screen number is zero.</li> <li><code>CTRL + a n</code>: (next) switches to the next window.</li> <li><code>CTRL + a p</code>: (prev) switches to the previous window.</li> <li><code>CTRL + a d</code>: (detach) detaches from a Screen</li> <li><code>CTRL + a A</code>: (title) rename the current window</li> <li><code>CTRL + a 0-9</code>: switches between windows 0 through 9.</li> <li><code>CTRL + a k</code> or <code>CTRL + d</code>: (kill) destroy the current window</li> <li><code>CTRL + a ?</code>: (help) display a list of all the command options available for Screen.</li> </ul>"},{"location":"tmux_screen/#persistent-terminal-sessions-using-tmux","title":"Persistent Terminal Sessions using Tmux","text":"<p>Tmux is a more modern equivalent to GNU screen.</p>"},{"location":"tmux_screen/#pre-requisite-screen-configuration-file-tmuxrc","title":"Pre-requisite: screen configuration file <code>~/.tmuxrc</code>","text":"<p>While not mandatory, we advise you to rely on our customized configuration file for tmux <code>.tmuxrc</code> available on Github.</p> <p>Otherwise, simply clone the ULHPC dotfile repository and make a symbolic link <code>~/.tmuxrc</code> targeting the file <code>tmux/tmuxrc</code> of the repository.</p>"},{"location":"tmux_screen/#tmux-commands","title":"Tmux commands","text":"<p>You can start a tmux session (i.e. creates a single window with a shell in it) with the <code>tmux</code> command. Its main command-lines options are listed below:</p> <ul> <li><code>tmux</code>: start a new tmux session</li> <li><code>tmux ls</code>: does not start tmux, but print the list of the existing sessions.</li> <li><code>tmux a</code>: resumes a detached tmux session</li> </ul> <p>Once within a tmux, you can invoke a tmux command which consist of a \"<code>CTRL + b</code>\" sequence followed by one other character. The main commands are:</p> <ul> <li><code>CTRL + b c</code>: (create) creates a new tmux window. The default tmux number is zero.</li> <li><code>CTRL + b n</code>: (next) switches to the next window.</li> <li><code>CTRL + b p</code>: (prev) switches to the previous window.</li> <li><code>CTRL + b d</code>: (detach) detaches from a session</li> <li><code>CTRL + b ,</code>: (title) rename the current window</li> <li><code>CTRL + b 0-9</code>: switches between windows 0 through 9.</li> <li><code>CTRL + d</code>: (kill) destroy the current window</li> <li><code>CTRL + b ?</code>: (help) display a list of all the command options available for tmux.</li> </ul>"},{"location":"writing/","title":"Developing SYCL programs for Intel\u00ae FPGA cards","text":""},{"location":"writing/#anatomy-of-a-sycl-program","title":"Anatomy of a SYCL program","text":""},{"location":"writing/#data-management","title":"Data Management","text":"<p>In the context of SYCL, Unified Shared Memory (USM) and buffers represent two different ways to handle memory and data management. They offer different levels of abstraction and ease of use, and the choice between them may depend on the specific needs of an application. Here's a breakdown of the differences:</p>"},{"location":"writing/#unified-shared-memory-usm","title":"Unified Shared Memory (USM)","text":"<p>Unified Shared Memory is a feature that simplifies memory management by providing a shared memory space across the host and various devices, like CPUs, GPUs, and FPGAs. USM provides three different types of allocations:</p> <ol> <li>Device Allocations: Allocated memory is accessible only by the device.</li> <li>Host Allocations: Allocated memory is accessible by the host and can be accessed by devices. However, the allocated memory is stored on the host global memory. </li> <li>Shared Allocations: Allocated memory is accessible by both the host and devices. The allocated memory is present in both global memories and it is synchronized between host and device.</li> </ol> <p>USM allows for more straightforward coding, akin to standard C++ memory management, and may lead to code that is easier to write and maintain. </p> <p>FPGA support</p> <p>SYCL USM host allocations are only supported by some BSPs, such as the Intel\u00ae FPGA Programmable Acceleration Card (PAC) D5005 (previously known as Intel\u00ae FPGA Programmable Acceleration Card (PAC) with Intel\u00ae Stratix\u00ae 10 SX FPGA). Check with your BSP vendor to see if they support SYCL USM host allocations.</p> <p>Using SYCL, you can verify if you have access to the different features:</p> <p>Verify USM capabilities</p> <pre><code>if (!device.has(sycl::aspect::usm_shared_allocations)) {\n# Try to default to host allocation only\nif (!device.has(sycl::aspect::usm_host_allocations)) {\n# Default to device and explicit data movement\nstd::array&lt;int,N&gt; host_array;\nint *my_array = malloc_device&lt;int&gt;(N, Q);\n}else{\n# Ok my_array is located on host memory but transferred to device as needed\nint* my_array = malloc_host&lt;int&gt;(N, Q);\n}\n}else{\n# Ok my_array is located on both global memories and synchronized automatically \nint* shared_array = malloc_shared&lt;int&gt;(N, Q);\n}\n</code></pre> <p>That's not all</p> <ul> <li>Concurrent accesses and atomic modificationes are not necessarily available even if you have host and shared capabilities.</li> <li>You need to verify <code>aspect::usm_atomic_shared_allocations</code> and <code>aspect::usm_atomic_host_allocations</code>.</li> </ul> <p>Bittware 520N-MX</p> <p>The USM host allocations is not supported by some BSPs. We will therefore use explicit data movement</p> <p>Explicit USM</p> QuestionSolution <ul> <li>Go to the <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code></li> <li>Replace the original code with explicit USM code </li> <li>Verify your code using emulation</li> </ul> <pre><code>#include &lt;iostream&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\nvoid VectorAdd(const int *vec_a_in, const int *vec_b_in, int *vec_c_out,\nint len) {\nfor (int idx = 0; idx &lt; len; idx++) {\nint a_val = vec_a_in[idx];\nint b_val = vec_b_in[idx];\nint sum = a_val + b_val;\nvec_c_out[idx] = sum;\n}\n}\nconstexpr int kVectSize = 256;\nint main() {\nbool passed = true;\ntry {\n// Use compile-time macros to select either:\n//  - the FPGA emulator device (CPU emulation of the FPGA)\n//  - the FPGA device (a real FPGA)\n//  - the simulator device\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// create the device queue\nsycl::queue q(selector);\n// make sure the device supports USM host allocations\nauto device = q.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\n// declare arrays and fill them\nint host_vec_a[kVectSize];\nint host_vec_b[kVectSize];\nint host_vec_c[kVectSize];\nint * vec_a = malloc_device&lt;int&gt;(kVectSize,q);\nint * vec_b = malloc_device&lt;int&gt;(kVectSize,q);\nint * vec_c = malloc_device&lt;int&gt;(kVectSize,q);\nfor (int i = 0; i &lt; kVectSize; i++) {\nhost_vec_a[i] = i;\nhost_vec_b[i] = (kVectSize - i);\n}\nstd::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n{\nq.memcpy(vec_a, host_vec_a, kVectSize * sizeof(int)).wait();\nq.memcpy(vec_b, host_vec_b, kVectSize * sizeof(int)).wait();\nq.single_task&lt;VectorAddID&gt;([=]() {\nVectorAdd(vec_a, vec_b, vec_c, kVectSize);\n}).wait();\n}\n// result is copied back to host automatically when accessors go out of\n// scope.\nq.memcpy(host_vec_c, vec_c, kVectSize * sizeof(int)).wait();\n// verify that VC is correct\nfor (int i = 0; i &lt; kVectSize; i++) {\nint expected = host_vec_a[i] + host_vec_b[i];\nif (host_vec_c[i] != expected) {\nstd::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; host_vec_c[i] &lt;&lt; \", expected (\"\n&lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; host_vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; host_vec_b[i]\n&lt;&lt; std::endl;\npassed = false;\n}\n}\nstd::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\nsycl::free(vec_a,q);\nsycl::free(vec_b,q);\nsycl::free(vec_c,q);\n} catch (sycl::exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre>"},{"location":"writing/#buffer-accessors","title":"Buffer &amp; accessors","text":"<p>Buffers and accessors are key abstractions that enable memory management and data access across various types of devices like CPUs, GPUs, DSPs, etc.</p> <ol> <li> <p>Buffers:Buffers in SYCL are objects that represent a region of memory accessible by the runtime. They act as containers for data and provide a way to abstract the memory management across host and device memories. This allows for efficient data movement and optimization by the runtime, as it can manage the data movement between host and device memory transparently.</p> </li> <li> <p>Accessors:Accessors provide a way to access the data inside buffers. They define the type of access (read, write, read-write) and can be used within kernels to read from or write to buffers.</p> </li> </ol> <p>Advantage</p> <p>Through the utilization of these accessors, the SYCL runtime examines the interactions with the buffers and constructs a dependency graph that maps the relationship between host and device functions. This enables the runtime to automatically orchestrate the transfer of data and the sequencing of kernel activities.</p> <p>Using Buffers and Accessors</p> <pre><code>    #include &lt;array&gt; // oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\nclass Kernel;\nconstexpr int N = 100;\nstd::array&lt;int,N&gt; in;\nstd::array&lt;int,N&gt; out;\nfor (int i = 0 ; i &lt;N; i++)\nin[i] = i+1;\nqueue device_queue(sycl::ext::intel::fpga_selector_v);\n{ // This one is very important to define the buffer scope\n// buffer&lt;int, 1&gt; in_device_buf(in.data(), in.size());\n// Or more convenient\nbuffer in_device_buf(in_array);\nbuffer out_device_buf(out_array);\ndevice_queue.submit([&amp;](handler &amp;h) {\naccessor in(in_device_buf, h, read_only);\naccessor out(out_device_buf, h, write_only, no_init);\nh.single_task&lt;Kernel&gt;([=]() { });\n};\n} // Accessor going out of the scope\n// Data has been copied back !!!\n</code></pre> <p>What about memory accesses in FPGA ? </p> <p>For FPGAs, the access pattern, access width, and coalescing of memory accesses can significantly affect performance. You might want to make use of various attributes and pragmas specific to your compiler and FPGA to guide the compiler in optimizing memory accesses.</p> <ul> <li>The <code>vector_add.cpp</code> source code introduced in the compiling section relies on buffers and accessors. Although DPC++ is built on top of SYCL, the use of specific hardware needs some attentions</li> </ul> <p>Executing the FPGA bitstream</p> QuestionSolution <ul> <li>Go to <code>/project/home/p200117/FPGA</code></li> <li>We have build to different FPGA bitstream versions of <code>vector_add.cpp</code>:</li> <li>Go to the folder <code>01-no_data_alignment/src</code> and execute the code on the FPGA card. What do you see ?</li> <li>Now go to the folder <code>02-with_data_alignment/src</code> and execute the code on the FPGA card. How could we align data properly ?</li> </ul> <p><pre><code>Running on device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nadd two vectors of size 256\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from host to device because of lack of alignment\n**                 host ptr (0xb60b350) and/or dev offset (0x400) is not aligned to 64 bytes\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from host to device because of lack of alignment\n**                 host ptr (0xb611910) and/or dev offset (0x800) is not aligned to 64 bytes\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from device to host because of lack of alignment\n**                 host ptr (0xb611d20) and/or dev offset (0xc00) is not aligned to 64 bytes\nPASSED\n</code></pre> Replace the following lines: <pre><code>    int * vec_a = new int[kVectSize];\nint * vec_b = new int[kVectSize];\nint * vec_c = new int[kVectSize];\n</code></pre> by these ones: <pre><code>   int * vec_a = new(std::align_val_t{ 64 }) int[kVectSize];\nint * vec_b = new(std::align_val_t{ 64 }) int[kVectSize];\nint * vec_c = new(std::align_val_t{ 64 }) int[kVectSize]; </code></pre></p>"},{"location":"writing/#queue","title":"Queue","text":"<p>Contrary to OpenCL, queues in SYCL are out-of-order by default. Nonetheless, you can change this behavior you declare it in your code.</p> <p>In-order-queue</p> <p> <pre><code>  ... queue device_queue{sycl::ext::intel::fpga_selector_v};\n// Task A\ndevice_queue.submit([&amp;](handler&amp; h) {\nh.single_task&lt;TaskA&gt;([=]() { });\n});\n// Task B\ndevice_queue.submit([&amp;](handler&amp; h) {\nh.single_task&lt;TaskB&gt;([=]() { });\n});\n// Task C\ndevice_queue.submit([&amp;](handler&amp; h) {\nh.single_task&lt;TaskC&gt;([=]() { });\n}); ...\n</code></pre> <pre><code>graph TD\nA[TaskA] --&gt; B[TaskB];\nB[TaskB] --&gt; C[TaskC];</code></pre> </p> <p>This behavior is not very useful nor flexible. Queue objects, by default, are out-of-order queues, except when they're constructed with the in-order queue property. Because of this, they must include mechanisms to arrange tasks that are sent to them. The way queues organize tasks is by allowing the user to notify the runtime about the dependencies that exist between these tasks. These dependencies can be described in two ways: either explicitly or implicitly, through the use of command groups.</p> <p>A command group is a specific object that outlines a task and its dependencies. These groups are generally expressed as C++ lambdas and are handed over as arguments to the submit() method within a queue object. The single parameter within this lambda is a reference to a handler object, utilized inside the command group to define actions, generate accessors, and outline dependencies.</p>"},{"location":"writing/#explicit-dependencies","title":"Explicit dependencies","text":"<p>Like for OpenCL, you can manage dependencies explicitly using events. </p> <p>Using events</p> <p> <pre><code>  ... queue device_queue{sycl::ext::intel::fpga_selector_v};\n// Task A\nauto event_A = device_queue.submit([&amp;](handler &amp;h) {\nh.single_task&lt;TaskA&gt;([=]() { });\n});\nevent_A.wait();\n// Task B\nauto event_B = device_queue.submit([&amp;](handler &amp;h) {\nh.single_task&lt;TaskB&gt;([=]() { });\n});\n// Task C\nauto event_C = device_queue.submit([&amp;](handler &amp;h) {\nh.single_task&lt;TaskC&gt;([=]() { });\n});\n// Task D\ndevice_queue.submit([&amp;](handler &amp;h) {\nh.depends_on({eB, eC});\nh.parallel_for(N, [=](id&lt;1&gt; i) { /*...*/ });\n}).wait();\n...\n</code></pre> <pre><code>graph TD\nA[TaskA] --&gt; B[TaskB];\nA[TaskA] --&gt; C[TaskC];\nB[TaskB] --&gt; D[TaskD];\nC[TaskC] --&gt; D[TaskD];</code></pre> </p> <ul> <li>Explicit dependencies using events is relevant when you use USM since buffers make use of accessors to model data dependencies.</li> <li>They are three possibilities to declare a dependcies explicitely:</li> <li>Calling the method <code>wait()</code> on the queue it-self</li> <li>Calling the method <code>wait</code> on the event return by the queue after submitting a command</li> <li>Calling the method <code>depends_on</code> of the handler object</li> </ul>"},{"location":"writing/#implicit-dependencies","title":"Implicit dependencies","text":"<ul> <li>Implicit dependencies occurs when your are using buffer &amp; accessor.</li> <li> <p>Accessors have different access modes:</p> </li> <li> <p>read_only: The content of the buffer can only be accessed for reading. So the content will only be copied once to the device</p> </li> <li>write_only: The content of the buffer can only be accessed for writing. The content of buffer is still copied from host to device before the kernel starts </li> <li>read_write: The content of the buffer can be accessed for reading and writing.</li> </ul> <p>You can add the <code>no_init</code> property to an accessor in <code>write_only</code> mode. This tells the runtime that the original data contains in the buffer can be ignored and don't need to be copied from host to device.</p> <p>Implicit dependencies obey to three main patterns (see DPC++ book):</p> <ul> <li>Read-after-Write  (RAW) : occurs when some data modified by a kernel should be read by another kernel. </li> <li>Write-after-Read  (WAR) : occurs when some data read by a kernel will be modified by another one</li> <li>Write-after-Write (WAW) : occurs when two kernels modified the same data</li> </ul> <p>Implicit dependencies</p> QuestionSolution <ul> <li>By default without access mode, each accessor will be read_write inducing unnecessary copies.</li> <li>Note also the first use of <code>host_accessor</code>. Why did we use it here ?</li> <li>Modifiy the following code to take into account implicit dependencies.  <pre><code>   constexpr int N = 100;\nqueue Q;\nbuffer&lt;int&gt; A{range{N}};\nbuffer&lt;int&gt; B{range{N}};\nbuffer&lt;int&gt; C{range{N}};\nQ.submit([&amp;](handler &amp;h) {\naccessor aA{A, h};\naccessor aB{B, h};\naccessor aC{C, h};\nh.single_task&lt;Kernel1&gt;([=]() { for(unsigned int i =0; i&lt;N; i++)\naA[i] = 10;\naB[i] = 50;\naC[i] = 0;\n});\n});\nQ.submit([&amp;](handler &amp;h) {\naccessor aA{A, h};\naccessor aB{B, h};\naccessor aC{C, h};\nh.single_task&lt;Kernel2&gt;([=]() { for(unsigned int i =0; i&lt;N; i++)\naC[i] += aA[i] + aB[i]; });\n});\nQ.submit([&amp;](handler &amp;h) {\naccessor aC{C, h};\nh.single_task&lt;Kernel3&gt;([=]() {\nfor(unsigned int i =0; i&lt;N; i++)\naC[i]++; });\n});\nhost_accessor result{C};\n</code></pre></li> </ul> <pre><code>   constexpr int N = 100;\nqueue Q;\nbuffer&lt;int&gt; A{range{N}};\nbuffer&lt;int&gt; B{range{N}};\nbuffer&lt;int&gt; C{range{N}};\nQ.submit([&amp;](handler &amp;h) {\naccessor aA{A, h, write_only, no_init};\naccessor aB{B, h, write_only, no_init};\naccessor aC{C, h, write_only, no_init};\nh.single_task&lt;Kernel1&gt;([=]() { for(unsigned int i =0; i&lt;N; i++)\naA[i] = 10;\naB[i] = 50;\naC[i] = 0;\n});\n});\nQ.submit([&amp;](handler &amp;h) {\naccessor aA{A, h, read_only};\naccessor aB{B, h, read_only};\naccessor aC{C, h, write_only};\nh.single_task&lt;Kernel2&gt;([=]() { for(unsigned int i =0; i&lt;N; i++)\naC[i] += aA[i] + aB[i]; });\n});\nQ.submit([&amp;](handler &amp;h) {\naccessor aC{C, h, write_only};\nh.single_task&lt;Kernel3&gt;([=]() {\nfor(unsigned int i =0; i&lt;N; i++)\naC[i]++; });\n});\nhost_accessor result{C, read_only};\n</code></pre>"},{"location":"writing/#parallelism-model-for-fpga","title":"Parallelism model for FPGA","text":"<ul> <li>FPGA strongly differs from ISA-based hardware such as CPU and GPU</li> </ul> <p>Difference between Instruction Set architecture and Spatial architecture</p> Instruction Set ArchitectureSpatial Architecture <ul> <li>Made for general-purpose computation: hardware is constantly reused </li> <li>Workflow constrained by a set of pre-defined units (Control Units, ALUs, registers)</li> <li>Data/Register size are fixed</li> <li>Different instruction executed in each clock cycle : temporal execution </li> </ul> <ul> <li>Keep only what it needs -- the hardware can be reconfigured</li> <li>Specialize the everything by unrolling the hardware: spatial execution</li> <li>Each operation uses a different hardware region</li> <li>The design can take more space than the FPGA offers </li> </ul> <p></p> <ul> <li> <p>The most obvious source of parallelism for FPGA is pipelining by inserting registers to store each operation output and keep all hardware unit busy. </p> </li> <li> <p>Pipelining parallelism has therefore many stages. </p> </li> <li> <p>If you don't have enough work to fill the pipeline, then the efficiency is very low.</p> </li> <li> <p>The authors of the DPC++ book have illustrated it perfectly in Chapter 17.</p> </li> </ul> <p>Pipelining example provided chap.17 (DPC++ book)</p> Processing a single element (Figure. 17-13)Taking advantage of pipelining (Figure 17-14) <p></p> <ul> <li>The pipeline is mostly empty.</li> <li>Hardware units are not busy and the efficiency is thus low.</li> </ul> <p></p> <ul> <li>More data than stages, the pipeline is full and all hardware units are busy.</li> </ul> <p>Vectorization</p> <p>Vectorization is not the main source of parallelism but help designing efficient pipeline. Since hardware can be reconfigured at will. The offline compiler can design N-bits Adders, multipliers which simplify greatly vectorization. In fact, the offline compiler vectorizes your design automatically if possible.</p>"},{"location":"writing/#pipelining-with-nd-range-kernels","title":"Pipelining with ND-range kernels","text":"<ul> <li>ND-range kernels are based on a hierachical grouping of work-items</li> <li>A work-item represents a single unit of work </li> <li>Independent simple units of work don't communicate or share data very often</li> <li>Useful when porting a GPU kernel to FPGA</li> </ul> DPC++ book -- Figure 17-15  <ul> <li>FPGAs are different from GPU (lots of thread started at the same time)</li> <li>Impossible to replicate a hardware for a million of work-items</li> <li>Work-items are injected into the pipeline</li> <li>A deep pipeline means lots of work-items executing different tasks in parallel</li> </ul> DPC++ book -- Figure 17-16  <ul> <li>In order to write basic data-parallel kernel, you will need to use the <code>parallel_for</code> method. Below is an example of simple data-parallel kernel. As you can notice it, there is no notion of groups nor sub-groups. </li> </ul> <p>Matrix addition</p> <pre><code>   constexpr int N = 2048;\nconstexpr int M = 1024;\nqueue.submit([&amp;](sycl::handler &amp;h) {\nsycl::accessor acc_a{buffer_a, h, sycl::read_only};\nsycl::accessor acc_b{buffer_b, h, sycl::read_only};\nsycl::accessor acc_c{buffer_c, h, sycl::read_write, sycl::no_init};\nh.parallel_for(range{N, M}, [=](sycl::id&lt;2&gt; idx) {\nacc_c[idx] = acc_a[idx] + acc_b[idx];\n});\n});\n</code></pre> <p>Vector addition</p> QuestionSolution <ul> <li>Go to the <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code></li> <li>Adapt the <code>vector_add.cpp</code> single-task kernel to a basis data-parallel kernel</li> <li>Emulate to verify your design</li> </ul> <pre><code>#include &lt;iostream&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\nconstexpr int kVectSize = 256;\nint main() {\nbool passed = true;\ntry {\n// Use compile-time macros to select either:\n//  - the FPGA emulator device (CPU emulation of the FPGA)\n//  - the FPGA device (a real FPGA)\n//  - the simulator device\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// create the device queue\nsycl::queue q(selector);\n// make sure the device supports USM host allocations\nauto device = q.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\n// declare arrays and fill them\nint * vec_a = new(std::align_val_t{ 64 }) int[kVectSize];\nint * vec_b = new(std::align_val_t{ 64 }) int[kVectSize];\nint * vec_c = new(std::align_val_t{ 64 }) int[kVectSize];\nfor (int i = 0; i &lt; kVectSize; i++) {\nvec_a[i] = i;\nvec_b[i] = (kVectSize - i);\n}\nstd::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n{\n// copy the input arrays to buffers to share with kernel\nsycl::buffer buffer_a{vec_a, sycl::range(kVectSize)};\nsycl::buffer buffer_b{vec_b, sycl::range(kVectSize)};\nsycl::buffer buffer_c{vec_c, sycl::range(kVectSize)};\nq.submit([&amp;](sycl::handler &amp;h) {\n// use accessors to interact with buffers from device code\nsycl::accessor accessor_a{buffer_a, h, sycl::read_only};\nsycl::accessor accessor_b{buffer_b, h, sycl::read_only};\nsycl::accessor accessor_c{buffer_c, h, sycl::write_only, sycl::no_init};\nh.parallel_for&lt;VectorAddID&gt;(sycl::range(kVectSize),[=](sycl::id&lt;1&gt; idx) {\naccessor_c[idx] = accessor_a[idx] + accessor_b[idx];\n});\n});\n}\n// result is copied back to host automatically when accessors go out of\n// scope.\n// verify that VC is correct\nfor (int i = 0; i &lt; kVectSize; i++) {\nint expected = vec_a[i] + vec_b[i];\nif (vec_c[i] != expected) {\nstd::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; vec_c[i] &lt;&lt; \", expected (\"\n&lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; vec_b[i]\n&lt;&lt; std::endl;\npassed = false;\n}\n}\nstd::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\ndelete[] vec_a;\ndelete[] vec_b;\ndelete[] vec_c;\n} catch (sycl::exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>If you want to have a fine-grained control of your data-parallel kernel, ND-range data-parallel kernels are the equivalent of ND-range kernels in OpenCL. </li> </ul> <p>ND-range kernel in SYCL</p> <ul> <li><code>nd_range(range&lt;dimensions&gt; globalSize, range&lt;dimensions&gt; localSize);</code></li> <li>ND-range kernels are defined with two range objects<ul> <li>global representing the total size of work-items</li> <li>local representing the size of work-groups</li> </ul> </li> </ul> <p>Tiled Matrix Multiplication</p> QuestionSolution <ul> <li>Fill the blank and complete the code  <pre><code>#include &lt;iostream&gt;\n#include &lt;algorithm&gt;\n#include &lt;random&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;boost/align/aligned_allocator.hpp&gt;\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass MatMultKernel;\nint main() {\nbool passed = true;\ntry {\n// Use compile-time macros to select either:\n//  - the FPGA emulator device (CPU emulation of the FPGA)\n//  - the FPGA device (a real FPGA)\n//  - the simulator device\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// create the device queue\nsycl::queue q(selector);\n// make sure the device supports USM host allocations\nauto device = q.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\n// initialize input and output memory on the host\nconstexpr size_t N = 512;\nconstexpr size_t B =  16;\nstd::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_a(N * N);\nstd::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_b(N * N);\nstd::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_c(N * N); std::random_device rd;\nstd::mt19937 mt(rd());\nstd::uniform_real_distribution&lt;float&gt; dist(0.0, 1.0);\n// Generate random values\nstd::generate(mat_a.begin(), mat_a.end(), [&amp;dist, &amp;mt]() {\nreturn dist(mt);\n});\n// Generate random values\nstd::generate(mat_b.begin(), mat_b.end(), [&amp;dist, &amp;mt]() {\nreturn dist(mt);\n});\n// fill with zero\nstd::fill(mat_c.begin(), mat_c.end(), 0.0); std::cout &lt;&lt; \"Matrix multiplication A X B = C \" &lt;&lt;std::endl;\n{\n// copy the input arrays to buffers to share with kernel\n// We can access the buffer using mat[i][j]\nsycl::buffer&lt;float,2&gt; buffer_a{mat_a.data(), sycl::range&lt;2&gt;(N,N)};\nsycl::buffer&lt;float,2&gt; buffer_b{mat_b.data(), sycl::range&lt;2&gt;(N,N)};\nsycl::buffer&lt;float,2&gt; buffer_c{mat_c.data(), sycl::range&lt;2&gt;(N,N)};\n/* DEFINE HERE the global size and local size ranges*/\nq.submit([&amp;](sycl::handler &amp;h) {\n// use accessors to interact with buffers from device code\nsycl::accessor accessor_a{buffer_a, h, sycl::read_only};\nsycl::accessor accessor_b{buffer_b, h, sycl::read_only};\nsycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\nsycl::local_accessor&lt;float,2&gt; tileA{{B,B}, h};\nsycl::local_accessor&lt;float,2&gt; tileB{{B,B}, h};\nh.parallel_for&lt;MatMultKernel&gt;(sycl::nd_range{global, local}, [=](sycl::nd_item&lt;2&gt; item)\n[[intel::max_work_group_size(1, B, B)]]    {\n// Indices in the global index space:\nint m = item.get_global_id()[0];\nint n = item.get_global_id()[1];\n// Index in the local index space:\n// Provide local indexes i and j -- fill here\nfloat sum = 0;\nfor (int p = 0; p &lt; N/B; p++) {\n// Load the matrix tile from matrix A, and synchronize\n// to ensure all work-items have a consistent view\n// of the matrix tile in local memory.\ntileA[i][j] = accessor_a[m][p*B+j];\n// Do the same for tileB\n// fill here \nitem.barrier();\n// Perform computation using the local memory tile, and\n// matrix B in global memory.\nfor (int kk = 0; kk &lt; B; kk++) {\nsum += tileA[i][kk] * tileB[kk][j];\n}\n// After computation, synchronize again, to ensure all\n// Fill here \n}\n// Write the final result to global memory.\naccessor_c[m][n] = sum;\n});\n});\n}\n// result is copied back to host automatically when accessors go out of\n// scope.\n// verify that Matrix multiplication is correct\nfor (int i = 0; i &lt; N; i++) {\nfor (int j = 0; j &lt; N; j++){\nfloat true_val=0.0;\nfor (int k = 0 ; k &lt; N; k++){\ntrue_val += mat_a[i*N +k] * mat_b[k*N+j];\n}\nif (std::abs(true_val - mat_c[i*N+j])/true_val &gt; 1.0e-4 ) {\nstd::cout &lt;&lt; \"C[\" &lt;&lt; i &lt;&lt; \";\" &lt;&lt; j &lt;&lt; \"] = \" &lt;&lt; mat_c[i*N+j] &lt;&lt; \" expected = \" &lt;&lt; true_val &lt;&lt; std::endl;\npassed = false;\n}\n}\n}\nstd::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n} catch (sycl::exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre></li> </ul> <pre><code>#include &lt;iostream&gt;\n#include &lt;algorithm&gt;\n#include &lt;random&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;boost/align/aligned_allocator.hpp&gt;\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass MatMultKernel;\nint main() {\nbool passed = true;\ntry {\n// Use compile-time macros to select either:\n//  - the FPGA emulator device (CPU emulation of the FPGA)\n//  - the FPGA device (a real FPGA)\n//  - the simulator device\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// create the device queue\nsycl::queue q(selector);\n// make sure the device supports USM host allocations\nauto device = q.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\n// initialize input and output memory on the host\nconstexpr size_t N = 512;\nconstexpr size_t B =  16;\nstd::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_a(N * N);\nstd::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_b(N * N);\nstd::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_c(N * N); std::random_device rd;\nstd::mt19937 mt(rd());\nstd::uniform_real_distribution&lt;float&gt; dist(0.0, 1.0);\n// Generate random values\nstd::generate(mat_a.begin(), mat_a.end(), [&amp;dist, &amp;mt]() {\nreturn dist(mt);\n});\n// Generate random values\nstd::generate(mat_b.begin(), mat_b.end(), [&amp;dist, &amp;mt]() {\nreturn dist(mt);\n});\n// fill with zero\nstd::fill(mat_c.begin(), mat_c.end(), 0.0); std::cout &lt;&lt; \"Matrix multiplication A X B = C \" &lt;&lt;std::endl;\n{\n// copy the input arrays to buffers to share with kernel\n// We can access the buffer using mat[i][j]\nsycl::buffer&lt;float,2&gt; buffer_a{mat_a.data(), sycl::range&lt;2&gt;(N,N)};\nsycl::buffer&lt;float,2&gt; buffer_b{mat_b.data(), sycl::range&lt;2&gt;(N,N)};\nsycl::buffer&lt;float,2&gt; buffer_c{mat_c.data(), sycl::range&lt;2&gt;(N,N)};\nsycl::range global {N,N};\nsycl::range local  {B,B}; q.submit([&amp;](sycl::handler &amp;h) {\n// use accessors to interact with buffers from device code\nsycl::accessor accessor_a{buffer_a, h, sycl::read_only};\nsycl::accessor accessor_b{buffer_b, h, sycl::read_only};\nsycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\nsycl::local_accessor&lt;float,2&gt; tileA{{B,B}, h};\nsycl::local_accessor&lt;float,2&gt; tileB{{B,B}, h};\nh.parallel_for&lt;MatMultKernel&gt;(sycl::nd_range{global, local}, [=](sycl::nd_item&lt;2&gt; item)\n[[intel::max_work_group_size(1, B, B)]]    {\n// Indices in the global index space:\nint m = item.get_global_id()[0];\nint n = item.get_global_id()[1];\n// Index in the local index space:\nint i = item.get_local_id()[0];\nint j = item.get_local_id()[1];\nfloat sum = 0;\nfor (int p = 0; p &lt; N/B; p++) {\n// Load the matrix tile from matrix A, and synchronize\n// to ensure all work-items have a consistent view\n// of the matrix tile in local memory.\ntileA[i][j] = accessor_a[m][p*B+j];\ntileB[i][j] = accessor_b[p*B+i][n];\nitem.barrier();\n// Perform computation using the local memory tile, and\n// matrix B in global memory.\nfor (int kk = 0; kk &lt; B; kk++) {\nsum += tileA[i][kk] * tileB[kk][j];\n}\n// After computation, synchronize again, to ensure all\n// reads from the local memory tile are complete.\nitem.barrier();\n}\n// Write the final result to global memory.\naccessor_c[m][n] = sum;\n});\n});\n}\n// result is copied back to host automatically when accessors go out of\n// scope.\n// verify that Matrix multiplication is correct\nfor (int i = 0; i &lt; N; i++) {\nfor (int j = 0; j &lt; N; j++){\nfloat true_val=0.0;\nfor (int k = 0 ; k &lt; N; k++){\ntrue_val += mat_a[i*N +k] * mat_b[k*N+j];\n}\nif (std::abs(true_val - mat_c[i*N+j])/true_val &gt; 1.0e-4 ) {\nstd::cout &lt;&lt; \"C[\" &lt;&lt; i &lt;&lt; \";\" &lt;&lt; j &lt;&lt; \"] = \" &lt;&lt; mat_c[i*N+j] &lt;&lt; \" expected = \" &lt;&lt; true_val &lt;&lt; std::endl;\npassed = false;\n}\n}\n}\nstd::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n} catch (sycl::exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <p>Warning on work-items group size</p> <ul> <li>If the attribute [[intel::max_work_group_size(Z, Y, X)]] is not specified in your kernel, the workgroup size assumes a default value depending on compilation time and runtime constraints</li> <li>If your kernel contains a barrier, the Intel\u00ae oneAPI DPC++/C++ Compiler sets a default maximum scalarized work-group size of 128 work-items ==&gt; without this attribute, the previous ND-Range kernel would have failed since we have a local work-group size of B x B = 256 work-items </li> </ul>"},{"location":"writing/#pipelining-with-single-work-item-loop","title":"Pipelining with single-work item (loop)","text":"<ul> <li>When your code can't be decomposed into independent works, you can rely on loop parallelism using FPGA</li> <li>In such a situation, the pipeline inputs is not work-items but loop iterations</li> <li>For single-work-item kernels, the programmer need not do anything special to preserve the data dependency </li> <li>Communications between kernels is also much easier</li> </ul> DPC++ book -- Figure 17-21  <ul> <li>FPGA can efficiently handle loop execution, often maintaining a fully occupied pipeline or providing reports on what changes are necessary to enhance occupancy.</li> <li>It's evident that if loop iterations were substituted with work-items, where the value created by one work-item would have to be transferred to another for incremental computation, the algorithm's description would become far more complex.</li> </ul> <p>Single-work item creation</p> <ul> <li>Replace the <code>parallel_for</code>method by the <code>single_task</code> method defined in the handler class to create a single-work item kernel</li> <li>The source file <code>vector_add.cpp</code> from <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code> uses loop pipelining.</li> </ul> <pre><code>  #include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\nusing namespace sycl;\nint main(){\n// queue creation &amp; data initialization\nq.submit([&amp;](handler &amp;h) {\nh.single_task&lt;class MyKernel&gt;([=]() {\n// Code to be executed as a single task\n});\n});\nq.wait();\n}\n</code></pre> <p>Inferring a shift register -- the accumulator case</p> ProblemQuestionSolution <ul> <li>The following code sums double precision floating-point array</li> <li>The problem is the following one:</li> <li>For each loop iteration, the Intel\u00ae oneAPI DPC++/C++ Compiler takes &gt;1 cycles to compute the result of the addition and then stores it in the variable temp_sum</li> <li>So you have a data dependency on temp_sum  <pre><code>#include &lt;iostream&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass Accumulator;\nconstexpr int kVectSize = 256;\nint main() {\nbool passed = true;\ntry {\n// Use compile-time macros to select either:\n//  - the FPGA emulator device (CPU emulation of the FPGA)\n//  - the FPGA device (a real FPGA)\n//  - the simulator device\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// create the device queue\nsycl::queue q(selector);\n// make sure the device supports USM host allocations\nauto device = q.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\n// declare arrays and fill them\ndouble * vec = new(std::align_val_t{ 64 }) double[kVectSize];\ndouble res = 0;\nfor (int i = 0; i &lt; kVectSize; i++) {\nvec[i] = 1.0;\n}\nstd::cout &lt;&lt; \"Accumulate values \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n{\n// copy the input arrays to buffers to share with kernel\nsycl::buffer buffer_in{vec, sycl::range(kVectSize)};\nsycl::buffer buffer_out{&amp;res, sycl::range(1)};\nq.submit([&amp;](sycl::handler &amp;h) {\n// use accessors to interact with buffers from device code\nsycl::accessor arr{buffer_in, h, sycl::read_only};\nsycl::accessor result{buffer_out, h, sycl::write_only,sycl::no_init};\nh.single_task&lt;Accumulator&gt;([=]() {\ndouble temp_sum = 0;\nfor (int i = 0; i &lt; kVectSize; ++i)\ntemp_sum += arr[i];\nresult[0] = temp_sum;\n});\n});\n}\n// result is copied back to host automatically when accessors go out of\n// scope.\n// verify that Accumulation is correct\ndouble expected = 0.0; for (int i = 0; i &lt; kVectSize; i++) expected += vec[i];\nif (res != expected) {\nstd::cout &lt;&lt; \"res = \" &lt;&lt; res &lt;&lt;  \", expected = \"\n&lt;&lt; expected &lt;&lt; std::endl;\npassed = false;\n}\nstd::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\ndelete[] vec;\n} catch (sycl::exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre></li> </ul> <ul> <li>The following code rely on a shift register to relax the data dependency</li> <li>Fill in the blank to complete the implementation  <pre><code>#include &lt;iostream&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass Accumulator;\nconstexpr int kVectSize = 256;\n// Initialization cycle (let us take a bit more than 10)\nconstexpr int II_CYCLES = 12;\nint main() {\nbool passed = true;\ntry {\n// Use compile-time macros to select either:\n//  - the FPGA emulator device (CPU emulation of the FPGA)\n//  - the FPGA device (a real FPGA)\n//  - the simulator device\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// create the device queue\nsycl::queue q(selector);\n// make sure the device supports USM host allocations\nauto device = q.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\n// declare arrays and fill them\ndouble * vec = new(std::align_val_t{ 64 }) double[kVectSize];\ndouble res = 0;\nfor (int i = 0; i &lt; kVectSize; i++) {\nvec[i] = 1.0;\n}\nstd::cout &lt;&lt; \"Accumulate values \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n{\n// copy the input arrays to buffers to share with kernel\nsycl::buffer buffer_in{vec, sycl::range(kVectSize)};\nsycl::buffer buffer_out{&amp;res, sycl::range(1)};\nq.submit([&amp;](sycl::handler &amp;h) {\n// use accessors to interact with buffers from device code\nsycl::accessor arr{buffer_in, h, sycl::read_only};\nsycl::accessor result{buffer_out, h, sycl::write_only,sycl::no_init};\nh.single_task&lt;Accumulator&gt;([=]() {\n//Create shift register with II_CYCLE+1 elements\ndouble shift_reg[II_CYCLES+1];\n//Initialize all elements of the register to 0\n//You must initialize the shift register \n// fill here\n//Iterate through every element of input array\nfor(int i = 0; i &lt; kVectSize; ++i){\n//Load ith element into end of shift register\n//if N &gt; II_CYCLE, add to shift_reg[0] to preserve values\nshift_reg[II_CYCLES] = shift_reg[0] + arr[i];\n#pragma unroll\n//Shift every element of shift register\n//Done in 1 cycle if using loop unrolling\n// fill here\n}\n//Sum every element of shift register\ndouble temp_sum = 0;\n#pragma unroll\nfor(int i = 0; i &lt; II_CYCLES; ++i){\ntemp_sum += shift_reg[i];\n}\nresult[0] = temp_sum;\n});\n});\n}\n// result is copied back to host automatically when accessors go out of\n// scope.\n// verify that Accumulation is correct\ndouble expected = 0.0; for (int i = 0; i &lt; kVectSize; i++) expected += vec[i];\nif (res != expected) {\nstd::cout &lt;&lt; \"res = \" &lt;&lt; res &lt;&lt;  \", expected = \"\n&lt;&lt; expected &lt;&lt; std::endl;\npassed = false;\n}\nstd::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\ndelete[] vec;\n} catch (sycl::exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre></li> </ul> <pre><code>#include &lt;iostream&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass Accumulator;\nconstexpr int kVectSize = 256;\n// Initialization cycle (let us take a bit more than 10)\nconstexpr int II_CYCLES = 12;\nint main() {\nbool passed = true;\ntry {\n// Use compile-time macros to select either:\n//  - the FPGA emulator device (CPU emulation of the FPGA)\n//  - the FPGA device (a real FPGA)\n//  - the simulator device\n#if FPGA_SIMULATOR\nauto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\nauto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\nauto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n// create the device queue\nsycl::queue q(selector);\n// make sure the device supports USM host allocations\nauto device = q.get_device();\nstd::cout &lt;&lt; \"Running on device: \"\n&lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n&lt;&lt; std::endl;\n// declare arrays and fill them\ndouble * vec = new(std::align_val_t{ 64 }) double[kVectSize];\ndouble res = 0;\nfor (int i = 0; i &lt; kVectSize; i++) {\nvec[i] = 1.0;\n}\nstd::cout &lt;&lt; \"Accumulate values \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n{\n// copy the input arrays to buffers to share with kernel\nsycl::buffer buffer_in{vec, sycl::range(kVectSize)};\nsycl::buffer buffer_out{&amp;res, sycl::range(1)};\nq.submit([&amp;](sycl::handler &amp;h) {\n// use accessors to interact with buffers from device code\nsycl::accessor arr{buffer_in, h, sycl::read_only};\nsycl::accessor result{buffer_out, h, sycl::write_only,sycl::no_init};\nh.single_task&lt;Accumulator&gt;([=]() {\n//Create shift register with II_CYCLE+1 elements\ndouble shift_reg[II_CYCLES+1];\n//Initialize all elements of the register to 0\n//You must initialize the shift register \nfor (int i = 0; i &lt; II_CYCLES + 1; i++) {\nshift_reg[i] = 0;\n}\n//Iterate through every element of input array\nfor(int i = 0; i &lt; kVectSize; ++i){\n//Load ith element into end of shift register\n//if N &gt; II_CYCLE, add to shift_reg[0] to preserve values\nshift_reg[II_CYCLES] = shift_reg[0] + arr[i];\n#pragma unroll\n//Shift every element of shift register\n//Done in 1 cycle if using loop unrolling\nfor(int j = 0; j &lt; II_CYCLES; ++j){\nshift_reg[j] = shift_reg[j + 1];\n}\n} //Sum every element of shift register\ndouble temp_sum = 0;\n#pragma unroll\nfor(int i = 0; i &lt; II_CYCLES; ++i){\ntemp_sum += shift_reg[i];\n}\nresult[0] = temp_sum;\n});\n});\n}\n// result is copied back to host automatically when accessors go out of\n// scope.\n// verify that Accumulation is correct\ndouble expected = 0.0; for (int i = 0; i &lt; kVectSize; i++) expected += vec[i];\nif (res != expected) {\nstd::cout &lt;&lt; \"res = \" &lt;&lt; res &lt;&lt;  \", expected = \"\n&lt;&lt; expected &lt;&lt; std::endl;\npassed = false;\n}\nstd::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\ndelete[] vec;\n} catch (sycl::exception const &amp;e) {\n// Catches exceptions in the host code.\nstd::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n// Most likely the runtime couldn't find FPGA hardware!\nif (e.code().value() == CL_DEVICE_NOT_FOUND) {\nstd::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n\"system has a correctly configured FPGA board.\\n\";\nstd::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\nstd::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n\"-DFPGA_EMULATOR.\\n\";\n}\nstd::terminate();\n}\nreturn passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre>"},{"location":"writing/#summary","title":"Summary","text":"<p>We have seen</p> <ul> <li>The anatomy of SYCL program</li> <li>How to manage data movement between host and device for FPGA<ul> <li>Explicit data movement with USM</li> <li>Implicit data movement with Buffers &amp; accessors  </li> </ul> </li> <li>How to manage data dependencies between kernels<ul> <li>Explicit dependencies with events</li> <li>Implicit dependencies using buffers access mode </li> </ul> </li> <li>How to define kernels and the importance of pipelining in FPGA<ul> <li>ND-range kernel created with the <code>parallel_for</code> method</li> <li>Single-work item kernel with the <code>single_task</code> method</li> </ul> </li> </ul> <p>We did not see</p> <ul> <li>Hierachical Parallels kernels</li> <li>Memory models and atomics</li> <li>The DPC++ Parallel STL</li> </ul>"}]}